{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"themes/landscape/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/landscape/source/js/script.js","path":"js/script.js","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":1,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"89bf8579941a1b5418efb93321cec22bf42af304","modified":1543646392814},{"_id":"themes/landscape/.gitignore","hash":"58d26d4b5f2f94c2d02a4e4a448088e4a2527c77","modified":1543646392815},{"_id":"themes/landscape/Gruntfile.js","hash":"71adaeaac1f3cc56e36c49d549b8d8a72235c9b9","modified":1543646392816},{"_id":"themes/landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1543646392816},{"_id":"themes/landscape/README.md","hash":"37fae88639ef60d63bd0de22314d7cc4c5d94b07","modified":1543646392816},{"_id":"themes/landscape/_config.yml","hash":"1c5d49810af75de541b4a9edc26767dcb731d5bc","modified":1543646392816},{"_id":"themes/landscape/package.json","hash":"544f21a0b2c7034998b36ae94dba6e3e0f39f228","modified":1543646392824},{"_id":"source/_posts/Demo-code-for-the-C-Programming-Language-Introduction.md","hash":"d1711c74b5c5f5130fe7803120f8b9fcd0d6aec1","modified":1543646392814},{"_id":"source/_posts/Black-screen-after-installing-Nvidia-drivers-on-Ubuntu-18-04-fix.md","hash":"eea5517461a6586d837e57d2411d6132d624cedf","modified":1543646392814},{"_id":"source/_posts/Interpreting-Marcus-Aurelius-1.md","hash":"426a6b9e28547e683bd9ca2cea60a1d5dd577ee8","modified":1561343235574},{"_id":"source/_posts/Interpreting-Marcus-Aurelius-2.md","hash":"60c40613a0a996a81ad14297f6469c5265d61764","modified":1561729517032},{"_id":"source/_posts/Interpreting-Marcus-Aurelius-3.md","hash":"ec42d718464ff220be61ad7d8d96ba718fd06614","modified":1561729077582},{"_id":"source/_posts/Project-Compozen-Build-Log-0.md","hash":"42891eb5f9e258a543e98a5eb0ded15a257b5f8e","modified":1544195829122},{"_id":"source/_posts/hello-world.md","hash":"bf50cf7fdac7cd7a8c10c79fe29d45be7db1d50d","modified":1543646392814},{"_id":"source/_posts/markov_chain.md","hash":"70c563e382f628b2acf8d75d73113dd82611446e","modified":1555830406639},{"_id":"source/about-me/index.md","hash":"0a7c5344be08c4ac36383e8f531cbe1703b5f7b1","modified":1543646392815},{"_id":"source/contact-me/index.md","hash":"e85e1ed75699df9d0c2980bf0fd825ae58ee6a76","modified":1543646392815},{"_id":"themes/landscape/languages/de.yml","hash":"3ebf0775abbee928c8d7bda943c191d166ded0d3","modified":1543646392816},{"_id":"themes/landscape/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1543646392816},{"_id":"themes/landscape/languages/es.yml","hash":"76edb1171b86532ef12cfd15f5f2c1ac3949f061","modified":1543646392817},{"_id":"themes/landscape/languages/fr.yml","hash":"415e1c580ced8e4ce20b3b0aeedc3610341c76fb","modified":1543646392817},{"_id":"themes/landscape/languages/ja.yml","hash":"a73e1b9c80fd6e930e2628b393bfe3fb716a21a9","modified":1543646392817},{"_id":"themes/landscape/languages/ko.yml","hash":"881d6a0a101706e0452af81c580218e0bfddd9cf","modified":1543646392817},{"_id":"themes/landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1543646392817},{"_id":"themes/landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1543646392817},{"_id":"themes/landscape/languages/pt.yml","hash":"57d07b75d434fbfc33b0ddb543021cb5f53318a8","modified":1543646392818},{"_id":"themes/landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1543646392818},{"_id":"themes/landscape/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1543646392818},{"_id":"themes/landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1543646392818},{"_id":"themes/landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1543646392823},{"_id":"themes/landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1543646392823},{"_id":"themes/landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1543646392823},{"_id":"themes/landscape/layout/layout.ejs","hash":"f155824ca6130080bb057fa3e868a743c69c4cf5","modified":1543646392823},{"_id":"themes/landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1543646392823},{"_id":"themes/landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1543646392824},{"_id":"themes/landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1543646392824},{"_id":"themes/landscape/scripts/fancybox.js","hash":"aa411cd072399df1ddc8e2181a3204678a5177d9","modified":1543646392824},{"_id":"themes/landscape/layout/_partial/after-footer.ejs","hash":"d0d753d39038284d52b10e5075979cc97db9cd20","modified":1543646392819},{"_id":"themes/landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1543646392819},{"_id":"themes/landscape/layout/_partial/archive.ejs","hash":"950ddd91db8718153b329b96dc14439ab8463ba5","modified":1543646392819},{"_id":"themes/landscape/layout/_partial/article.ejs","hash":"c4c835615d96a950d51fa2c3b5d64d0596534fed","modified":1543646392819},{"_id":"themes/landscape/layout/_partial/footer.ejs","hash":"93518893cf91287e797ebac543c560e2a63b8d0e","modified":1543646392819},{"_id":"themes/landscape/layout/_partial/gauges-analytics.ejs","hash":"aad6312ac197d6c5aaf2104ac863d7eba46b772a","modified":1543646392820},{"_id":"themes/landscape/layout/_partial/google-analytics.ejs","hash":"f921e7f9223d7c95165e0f835f353b2938e40c45","modified":1543646392820},{"_id":"themes/landscape/layout/_partial/head.ejs","hash":"5abf77aec957d9445fc71a8310252f0013c84578","modified":1543646392820},{"_id":"themes/landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1543646392820},{"_id":"themes/landscape/layout/_partial/header.ejs","hash":"7e749050be126eadbc42decfbea75124ae430413","modified":1543646392820},{"_id":"themes/landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1543646392822},{"_id":"themes/landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":1543646392822},{"_id":"themes/landscape/layout/_widget/recent_posts.ejs","hash":"0d4f064733f8b9e45c0ce131fe4a689d570c883a","modified":1543646392822},{"_id":"themes/landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":1543646392822},{"_id":"themes/landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":1543646392822},{"_id":"themes/landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":1543646392823},{"_id":"themes/landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1543646392824},{"_id":"themes/landscape/source/css/_variables.styl","hash":"628e307579ea46b5928424313993f17b8d729e92","modified":1543646392827},{"_id":"themes/landscape/source/css/style.styl","hash":"a70d9c44dac348d742702f6ba87e5bb3084d65db","modified":1543646392833},{"_id":"themes/landscape/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1543646392833},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1543646392834},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1543646392834},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1543646392834},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1543646392834},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1543646392834},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1543646392836},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1543646392837},{"_id":"themes/landscape/source/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1543646392837},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1543646392837},{"_id":"themes/landscape/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1543646392821},{"_id":"themes/landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1543646392821},{"_id":"themes/landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1543646392821},{"_id":"themes/landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1543646392821},{"_id":"themes/landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1543646392821},{"_id":"themes/landscape/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1543646392821},{"_id":"themes/landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1543646392825},{"_id":"themes/landscape/source/css/_partial/article.styl","hash":"10685f8787a79f79c9a26c2f943253450c498e3e","modified":1543646392825},{"_id":"themes/landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1543646392825},{"_id":"themes/landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1543646392825},{"_id":"themes/landscape/source/css/_partial/header.styl","hash":"85ab11e082f4dd86dde72bed653d57ec5381f30c","modified":1543646392825},{"_id":"themes/landscape/source/css/_partial/highlight.styl","hash":"bf4e7be1968dad495b04e83c95eac14c4d0ad7c0","modified":1543646392826},{"_id":"themes/landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1543646392826},{"_id":"themes/landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1543646392826},{"_id":"themes/landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":1543646392826},{"_id":"themes/landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1543646392826},{"_id":"themes/landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1543646392827},{"_id":"themes/landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1543646392827},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1543646392828},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1543646392828},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1543646392831},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1543646392835},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1543646392835},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1543646392835},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1543646392835},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1543646392836},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1543646392836},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1543646392830},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1543646392829},{"_id":"themes/landscape/source/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1543646392833},{"_id":"public/about-me/index.html","hash":"69f7eae32deea41e05a3b28a3f7f390b3b28d230","modified":1561729567744},{"_id":"public/contact-me/index.html","hash":"bece62c5a06af96109740e7c975771fc7e8eea47","modified":1561729567744},{"_id":"public/2019/06/28/Interpreting-Marcus-Aurelius-3/index.html","hash":"e980c18577ceff9b5fa042535eac2adbf8a8ce10","modified":1561729567745},{"_id":"public/2019/06/25/Interpreting-Marcus-Aurelius-2/index.html","hash":"8b41401f3e3cff261653a4e0001e42f901b44b20","modified":1561729567745},{"_id":"public/2019/06/24/Interpreting-Marcus-Aurelius-1/index.html","hash":"a0512a07d999e9b3e54a49d73611dec25b07f05a","modified":1561729567745},{"_id":"public/2018/12/03/Project-Compozen-Build-Log-0/index.html","hash":"2be6dcf1e3fb9baf814c9e825e2ffb118331c9a4","modified":1561729567745},{"_id":"public/2018/10/22/Demo-code-for-the-C-Programming-Language-Introduction/index.html","hash":"d1e0bcab02d22142e000940c932a217be23b0f5e","modified":1561729567745},{"_id":"public/2018/09/27/hello-world/index.html","hash":"1c07c25b39628cc953eebd0f25412d5b882ead69","modified":1561729567745},{"_id":"public/tags/c/index.html","hash":"324fdaf8ead9db05c1375bfc2af6273134621b44","modified":1561729567745},{"_id":"public/tags/gcc/index.html","hash":"3d36e997e01c2e79e06d6e096596f10849cc7374","modified":1561729567745},{"_id":"public/tags/compiler/index.html","hash":"3bcbd6522058d6e9a4de915de1f39b36e8159ab2","modified":1561729567745},{"_id":"public/tags/nvidia/index.html","hash":"5f6d55b9dc48ee1dd823f27e92da335a5c6ecbd1","modified":1561729567745},{"_id":"public/tags/drivers/index.html","hash":"86ed857c45faa9bcf7fc250948297a11a6ee38b2","modified":1561729567745},{"_id":"public/tags/ubuntu/index.html","hash":"951217df1a584c5e0dadbc538ea74302368f8aea","modified":1561729567745},{"_id":"public/tags/18-04/index.html","hash":"c21b17f264cc8885d7a3c4b7415902fc13367143","modified":1561729567745},{"_id":"public/tags/grub/index.html","hash":"220d8f928a4f6c965f488a30f45cb560b5458228","modified":1561729567745},{"_id":"public/tags/marcus-aurelius/index.html","hash":"55192f817a1acebb595e9493f83b16d82568e391","modified":1561729567746},{"_id":"public/tags/stoicism/index.html","hash":"60c087123d8c7d8dc59bb23b688d6341443e4269","modified":1561729567746},{"_id":"public/tags/stoic/index.html","hash":"ef05532ca80110f16d7ddba0fae132cbf9af9c84","modified":1561729567746},{"_id":"public/tags/node-js/index.html","hash":"6607800f13d107bf3fa2cd448b7b73f92f5479d3","modified":1561729567746},{"_id":"public/tags/cli/index.html","hash":"cc1ae1e7054825d15c1f2e16d8efcb5df632ec39","modified":1561729567746},{"_id":"public/tags/opensource/index.html","hash":"8f7c525c38e46afb2a12ab946b0bc1ad8970c9ba","modified":1561729567746},{"_id":"public/tags/docker/index.html","hash":"721a353d0c5bd2147fa385c4d22694ede5ed66f4","modified":1561729567746},{"_id":"public/tags/docker-compose/index.html","hash":"53d2f24040331da7c3d68f50ebeef0c065f5a8a9","modified":1561729567746},{"_id":"public/tags/python/index.html","hash":"8038cdb7ffbab619196f75473caad86d9a0083cf","modified":1561729567746},{"_id":"public/tags/numpy/index.html","hash":"2de8f757bc620ee4db983c5406f907aa098f9dbf","modified":1561729567746},{"_id":"public/tags/n-grams/index.html","hash":"bff0f93f1e3c720782142f0bbbcfd260bec1890e","modified":1561729567746},{"_id":"public/tags/nlp/index.html","hash":"cd258ea7fbecf941034903ae0e1dd7e1f1679ccf","modified":1561729567746},{"_id":"public/tags/markov-chains/index.html","hash":"00f07b1100455c899eacf5e857efcac5363abb55","modified":1561729567746},{"_id":"public/archives/index.html","hash":"b6dc7f1c400877a333dc72934627eef450263b34","modified":1561729567746},{"_id":"public/archives/2018/index.html","hash":"74261ac81b7d46aaa4cddb0ef71e0b13231c661d","modified":1561729567746},{"_id":"public/archives/2018/09/index.html","hash":"f5078c772fbaa843106c24655aa3d3964111b2d7","modified":1561729567746},{"_id":"public/archives/2018/10/index.html","hash":"3c76ee8723f0f1a257ee33eb1dbcb8e493dc5e74","modified":1561729567746},{"_id":"public/archives/2018/12/index.html","hash":"e7b24f64954d19835854e1edbab595127814b2bb","modified":1561729567746},{"_id":"public/archives/2019/index.html","hash":"81d7ac907e4d0480e1fbb937039af4b46dcf8eb8","modified":1561729567746},{"_id":"public/archives/2019/04/index.html","hash":"334dde2f1a07d89e3188b0425e7e4492495b1e53","modified":1561729567747},{"_id":"public/archives/2019/06/index.html","hash":"37d59150f52a8f4ac2addd19dbfb9ca2b8c70561","modified":1561729567747},{"_id":"public/2019/04/21/markov_chain/index.html","hash":"7bea2798878593b834989fdfc6401d48db193cd4","modified":1561729567747},{"_id":"public/2018/10/13/Black-screen-after-installing-Nvidia-drivers-on-Ubuntu-18-04-fix/index.html","hash":"6fbe1b8e0e025204f313e1c122656331fae84781","modified":1561729567747},{"_id":"public/index.html","hash":"25bfd2d216b4c48a55a2f51fb9ced4ae9c251f56","modified":1561729567747},{"_id":"public/CNAME","hash":"89bf8579941a1b5418efb93321cec22bf42af304","modified":1561729567749},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1561729567749},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1561729567750},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1561729567750},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1561729567750},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1561729567750},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1561729567750},{"_id":"public/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1561729567750},{"_id":"public/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1561729567750},{"_id":"public/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1561729567750},{"_id":"public/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1561729567750},{"_id":"public/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1561729568153},{"_id":"public/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1561729568159},{"_id":"public/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1561729568159},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1561729568159},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1561729568159},{"_id":"public/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1561729568159},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1561729568159},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1561729568159},{"_id":"public/css/style.css","hash":"5f8dadd37d0052c557061018fe6f568f64fced9b","modified":1561729568159},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1561729568169},{"_id":"public/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1561729568169},{"_id":"public/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1561729568175},{"_id":"public/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1561729568225}],"Category":[],"Data":[],"Page":[{"title":"About me","date":"2018-09-27T11:55:27.000Z","_content":"\nHi, I'm Kasey! I'm a lot of things, and right now, I might be exactly what you need.\n\n### A Computer Scientist\nWith a Bachelor of Science in Computer Science from the University of the Philippines Baguio and a Masters of Information Systems from the University of the Philippines Open University, I am committed to creating only the most efficient solutions. A lot of my skills have been honed in parallel with what the University teaches. I believe that even though fundamentals are important, you owe it to yourself to grow and continually learn beyond what is being taught. So if you're a computer science student, or in any other course that has programming in it, then I'd love for you to read my content!\n\n### A Maker\nWhat is a maker? Well, a maker is an individual who loves to tinker to understand how things work. From that understanding, a maker will then try to \"Do It Yourself\" to replicate, innovate and improve on a particular device. From my background as a computer scientist and a computer technician(I've been building desktop computers since I was in grade school), I have the skills in order to understand how devices fundamentally work and create my own little projects from them!\n\n### A GIS Specialist\nGIS stands for Geographic Information Systems. These are information systems that help you create, manage and process spatial data. After a few months as a remote backend developer when I just graduated from my Computer Science degree, a professor of mine who I worked for as an Android Developer/Student Assistant called me to serve the country as a Science Research Specialist for the Phil-LiDAR 1 project. It was a Department of Science and Technology(DOST) funded project where we created high resolution flood hazard maps to give local government units. The maps gave them a more updated and accurate way of disaster risk reduction and management during typhoons. \n\nWhat particularly did I do? Well, to name a few, I was the system administrator for the laboratory infrastructure, handled a lot of process automations, made improvements on the GIS workflows and acted as both a software/hardware architect and  developer for our in house developed weather stations and apps.\n\n### A Drone Pilot\nAlso during my Phil-LiDAR 1 days, we purchased a DJI Inspire 1 drone, and there were 2 drone pilots! I was one of them, and my co-pilot was my very beautiful, smart, talented girlfriend Eloise <3 I also own my own quadcopter, a tiny but very nimble Hubsan X4!\n\n### An Entrepreneur\nAfter Phil-LiDAR 1, my very beautiful, smart, talented girlfriend Eloise and I established Hackspace Makerspace and Cafe! It is a hybrid between a makerspace and a cafe. At Hackspace you can rent out our electronics modules if you want to learn how to tinker or prototype your next electronics project! We also have 3d printers where you can make physical objects from your 3d models. Last but definitely not the least, we offer lovely delicious food that are cooked to perfection! Try it, I dare you :D \n","source":"about-me/index.md","raw":"---\ntitle: About me\ndate: 2018-09-27 19:55:27\n---\n\nHi, I'm Kasey! I'm a lot of things, and right now, I might be exactly what you need.\n\n### A Computer Scientist\nWith a Bachelor of Science in Computer Science from the University of the Philippines Baguio and a Masters of Information Systems from the University of the Philippines Open University, I am committed to creating only the most efficient solutions. A lot of my skills have been honed in parallel with what the University teaches. I believe that even though fundamentals are important, you owe it to yourself to grow and continually learn beyond what is being taught. So if you're a computer science student, or in any other course that has programming in it, then I'd love for you to read my content!\n\n### A Maker\nWhat is a maker? Well, a maker is an individual who loves to tinker to understand how things work. From that understanding, a maker will then try to \"Do It Yourself\" to replicate, innovate and improve on a particular device. From my background as a computer scientist and a computer technician(I've been building desktop computers since I was in grade school), I have the skills in order to understand how devices fundamentally work and create my own little projects from them!\n\n### A GIS Specialist\nGIS stands for Geographic Information Systems. These are information systems that help you create, manage and process spatial data. After a few months as a remote backend developer when I just graduated from my Computer Science degree, a professor of mine who I worked for as an Android Developer/Student Assistant called me to serve the country as a Science Research Specialist for the Phil-LiDAR 1 project. It was a Department of Science and Technology(DOST) funded project where we created high resolution flood hazard maps to give local government units. The maps gave them a more updated and accurate way of disaster risk reduction and management during typhoons. \n\nWhat particularly did I do? Well, to name a few, I was the system administrator for the laboratory infrastructure, handled a lot of process automations, made improvements on the GIS workflows and acted as both a software/hardware architect and  developer for our in house developed weather stations and apps.\n\n### A Drone Pilot\nAlso during my Phil-LiDAR 1 days, we purchased a DJI Inspire 1 drone, and there were 2 drone pilots! I was one of them, and my co-pilot was my very beautiful, smart, talented girlfriend Eloise <3 I also own my own quadcopter, a tiny but very nimble Hubsan X4!\n\n### An Entrepreneur\nAfter Phil-LiDAR 1, my very beautiful, smart, talented girlfriend Eloise and I established Hackspace Makerspace and Cafe! It is a hybrid between a makerspace and a cafe. At Hackspace you can rent out our electronics modules if you want to learn how to tinker or prototype your next electronics project! We also have 3d printers where you can make physical objects from your 3d models. Last but definitely not the least, we offer lovely delicious food that are cooked to perfection! Try it, I dare you :D \n","updated":"2018-12-01T06:39:52.815Z","path":"about-me/index.html","comments":1,"layout":"page","_id":"cjxg5gmdk00010lfoi19v48nq","content":"<p>Hi, I&#x2019;m Kasey! I&#x2019;m a lot of things, and right now, I might be exactly what you need.</p>\n<h3 id=\"A-Computer-Scientist\"><a href=\"#A-Computer-Scientist\" class=\"headerlink\" title=\"A Computer Scientist\"></a>A Computer Scientist</h3><p>With a Bachelor of Science in Computer Science from the University of the Philippines Baguio and a Masters of Information Systems from the University of the Philippines Open University, I am committed to creating only the most efficient solutions. A lot of my skills have been honed in parallel with what the University teaches. I believe that even though fundamentals are important, you owe it to yourself to grow and continually learn beyond what is being taught. So if you&#x2019;re a computer science student, or in any other course that has programming in it, then I&#x2019;d love for you to read my content!</p>\n<h3 id=\"A-Maker\"><a href=\"#A-Maker\" class=\"headerlink\" title=\"A Maker\"></a>A Maker</h3><p>What is a maker? Well, a maker is an individual who loves to tinker to understand how things work. From that understanding, a maker will then try to &#x201C;Do It Yourself&#x201D; to replicate, innovate and improve on a particular device. From my background as a computer scientist and a computer technician(I&#x2019;ve been building desktop computers since I was in grade school), I have the skills in order to understand how devices fundamentally work and create my own little projects from them!</p>\n<h3 id=\"A-GIS-Specialist\"><a href=\"#A-GIS-Specialist\" class=\"headerlink\" title=\"A GIS Specialist\"></a>A GIS Specialist</h3><p>GIS stands for Geographic Information Systems. These are information systems that help you create, manage and process spatial data. After a few months as a remote backend developer when I just graduated from my Computer Science degree, a professor of mine who I worked for as an Android Developer/Student Assistant called me to serve the country as a Science Research Specialist for the Phil-LiDAR 1 project. It was a Department of Science and Technology(DOST) funded project where we created high resolution flood hazard maps to give local government units. The maps gave them a more updated and accurate way of disaster risk reduction and management during typhoons. </p>\n<p>What particularly did I do? Well, to name a few, I was the system administrator for the laboratory infrastructure, handled a lot of process automations, made improvements on the GIS workflows and acted as both a software/hardware architect and  developer for our in house developed weather stations and apps.</p>\n<h3 id=\"A-Drone-Pilot\"><a href=\"#A-Drone-Pilot\" class=\"headerlink\" title=\"A Drone Pilot\"></a>A Drone Pilot</h3><p>Also during my Phil-LiDAR 1 days, we purchased a DJI Inspire 1 drone, and there were 2 drone pilots! I was one of them, and my co-pilot was my very beautiful, smart, talented girlfriend Eloise &lt;3 I also own my own quadcopter, a tiny but very nimble Hubsan X4!</p>\n<h3 id=\"An-Entrepreneur\"><a href=\"#An-Entrepreneur\" class=\"headerlink\" title=\"An Entrepreneur\"></a>An Entrepreneur</h3><p>After Phil-LiDAR 1, my very beautiful, smart, talented girlfriend Eloise and I established Hackspace Makerspace and Cafe! It is a hybrid between a makerspace and a cafe. At Hackspace you can rent out our electronics modules if you want to learn how to tinker or prototype your next electronics project! We also have 3d printers where you can make physical objects from your 3d models. Last but definitely not the least, we offer lovely delicious food that are cooked to perfection! Try it, I dare you :D </p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{}},"excerpt":"","more":"<p>Hi, I’m Kasey! I’m a lot of things, and right now, I might be exactly what you need.</p>\n<h3 id=\"A-Computer-Scientist\"><a href=\"#A-Computer-Scientist\" class=\"headerlink\" title=\"A Computer Scientist\"></a>A Computer Scientist</h3><p>With a Bachelor of Science in Computer Science from the University of the Philippines Baguio and a Masters of Information Systems from the University of the Philippines Open University, I am committed to creating only the most efficient solutions. A lot of my skills have been honed in parallel with what the University teaches. I believe that even though fundamentals are important, you owe it to yourself to grow and continually learn beyond what is being taught. So if you’re a computer science student, or in any other course that has programming in it, then I’d love for you to read my content!</p>\n<h3 id=\"A-Maker\"><a href=\"#A-Maker\" class=\"headerlink\" title=\"A Maker\"></a>A Maker</h3><p>What is a maker? Well, a maker is an individual who loves to tinker to understand how things work. From that understanding, a maker will then try to “Do It Yourself” to replicate, innovate and improve on a particular device. From my background as a computer scientist and a computer technician(I’ve been building desktop computers since I was in grade school), I have the skills in order to understand how devices fundamentally work and create my own little projects from them!</p>\n<h3 id=\"A-GIS-Specialist\"><a href=\"#A-GIS-Specialist\" class=\"headerlink\" title=\"A GIS Specialist\"></a>A GIS Specialist</h3><p>GIS stands for Geographic Information Systems. These are information systems that help you create, manage and process spatial data. After a few months as a remote backend developer when I just graduated from my Computer Science degree, a professor of mine who I worked for as an Android Developer/Student Assistant called me to serve the country as a Science Research Specialist for the Phil-LiDAR 1 project. It was a Department of Science and Technology(DOST) funded project where we created high resolution flood hazard maps to give local government units. The maps gave them a more updated and accurate way of disaster risk reduction and management during typhoons. </p>\n<p>What particularly did I do? Well, to name a few, I was the system administrator for the laboratory infrastructure, handled a lot of process automations, made improvements on the GIS workflows and acted as both a software/hardware architect and  developer for our in house developed weather stations and apps.</p>\n<h3 id=\"A-Drone-Pilot\"><a href=\"#A-Drone-Pilot\" class=\"headerlink\" title=\"A Drone Pilot\"></a>A Drone Pilot</h3><p>Also during my Phil-LiDAR 1 days, we purchased a DJI Inspire 1 drone, and there were 2 drone pilots! I was one of them, and my co-pilot was my very beautiful, smart, talented girlfriend Eloise &lt;3 I also own my own quadcopter, a tiny but very nimble Hubsan X4!</p>\n<h3 id=\"An-Entrepreneur\"><a href=\"#An-Entrepreneur\" class=\"headerlink\" title=\"An Entrepreneur\"></a>An Entrepreneur</h3><p>After Phil-LiDAR 1, my very beautiful, smart, talented girlfriend Eloise and I established Hackspace Makerspace and Cafe! It is a hybrid between a makerspace and a cafe. At Hackspace you can rent out our electronics modules if you want to learn how to tinker or prototype your next electronics project! We also have 3d printers where you can make physical objects from your 3d models. Last but definitely not the least, we offer lovely delicious food that are cooked to perfection! Try it, I dare you :D </p>\n"},{"title":"Contact me","date":"2018-09-27T11:55:37.000Z","_content":"Still under construction :p\n\nIf you have any inquries, or if you want to work with me (which is very fun), send me an email:\n\nkasey.hackspace@gmail.com","source":"contact-me/index.md","raw":"---\ntitle: Contact me\ndate: 2018-09-27 19:55:37\n---\nStill under construction :p\n\nIf you have any inquries, or if you want to work with me (which is very fun), send me an email:\n\nkasey.hackspace@gmail.com","updated":"2018-12-01T06:39:52.815Z","path":"contact-me/index.html","comments":1,"layout":"page","_id":"cjxg5gmdm00030lfo0ppgd397","content":"<p>Still under construction :p</p>\n<p>If you have any inquries, or if you want to work with me (which is very fun), send me an email:</p>\n<p><a href=\"mailto:kasey.hackspace@gmail.com\" target=\"_blank\" rel=\"noopener\">kasey.hackspace@gmail.com</a></p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{}},"excerpt":"","more":"<p>Still under construction :p</p>\n<p>If you have any inquries, or if you want to work with me (which is very fun), send me an email:</p>\n<p><a href=\"mailto:kasey.hackspace@gmail.com\" target=\"_blank\" rel=\"noopener\">kasey.hackspace@gmail.com</a></p>\n"}],"Post":[{"title":"Demo code for the C Programming Language Introduction","date":"2018-10-22T07:17:34.000Z","_content":"Last week we conducted a basic C Programming workshop in collaboration with [Mechatronics SLU](https://www.facebook.com/MechatronicsSLU) at Saint Louis University, Baguio! It was a cool experience, and I really wanted to make sure that the students really understood the concepts we taught. After the session, we promised them that we would upload all the demo code we made for them to study and practise on :)\n\nHere's the main repository:\nhttps://github.com/kaseyhackspace/c_demo\n\nIf you don't know how to use `git` and github.com, don't worry, I'll make a post about that one of these days when I get the time!\n\nIn the meantime, here are links per chapter/topic so that you guys don't have to learn git right away :). Once you are redirected to the branch, go ahead and click on `main.c` in order to view the source code for that topic.\n\n1.  [Command line arguments](https://github.com/kaseyhackspace/c_demo/tree/1_cmd_line_arguments)\n2. [Input Output](https://github.com/kaseyhackspace/c_demo/tree/2_input_output)\n3. [Arithmetic and Conditional Operators](https://github.com/kaseyhackspace/c_demo/tree/3_operators)\n4. [Conditional Statements](https://github.com/kaseyhackspace/c_demo/tree/4_conditional)\n5. [The Switch Statement](https://github.com/kaseyhackspace/c_demo/tree/5_switch)\n6. [Loops](https://github.com/kaseyhackspace/c_demo/tree/6_loops)\n7. [Functions](https://github.com/kaseyhackspace/c_demo/tree/7_functions)\n\nThere you have it, all the source code for last week's C programming workshop! Stay tuned as I'll be discussing each source code in detail in the days to come :)","source":"_posts/Demo-code-for-the-C-Programming-Language-Introduction.md","raw":"---\ntitle: Demo code for the C Programming Language Introduction\ndate: 2018-10-22 15:17:34\ntags: \n    - c \n    - c++\n    - gcc\n    - compiler\n---\nLast week we conducted a basic C Programming workshop in collaboration with [Mechatronics SLU](https://www.facebook.com/MechatronicsSLU) at Saint Louis University, Baguio! It was a cool experience, and I really wanted to make sure that the students really understood the concepts we taught. After the session, we promised them that we would upload all the demo code we made for them to study and practise on :)\n\nHere's the main repository:\nhttps://github.com/kaseyhackspace/c_demo\n\nIf you don't know how to use `git` and github.com, don't worry, I'll make a post about that one of these days when I get the time!\n\nIn the meantime, here are links per chapter/topic so that you guys don't have to learn git right away :). Once you are redirected to the branch, go ahead and click on `main.c` in order to view the source code for that topic.\n\n1.  [Command line arguments](https://github.com/kaseyhackspace/c_demo/tree/1_cmd_line_arguments)\n2. [Input Output](https://github.com/kaseyhackspace/c_demo/tree/2_input_output)\n3. [Arithmetic and Conditional Operators](https://github.com/kaseyhackspace/c_demo/tree/3_operators)\n4. [Conditional Statements](https://github.com/kaseyhackspace/c_demo/tree/4_conditional)\n5. [The Switch Statement](https://github.com/kaseyhackspace/c_demo/tree/5_switch)\n6. [Loops](https://github.com/kaseyhackspace/c_demo/tree/6_loops)\n7. [Functions](https://github.com/kaseyhackspace/c_demo/tree/7_functions)\n\nThere you have it, all the source code for last week's C programming workshop! Stay tuned as I'll be discussing each source code in detail in the days to come :)","slug":"Demo-code-for-the-C-Programming-Language-Introduction","published":1,"updated":"2018-12-01T06:39:52.814Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxg5gmdg00000lforwqpztre","content":"<p>Last week we conducted a basic C Programming workshop in collaboration with <a href=\"https://www.facebook.com/MechatronicsSLU\" target=\"_blank\" rel=\"noopener\">Mechatronics SLU</a> at Saint Louis University, Baguio! It was a cool experience, and I really wanted to make sure that the students really understood the concepts we taught. After the session, we promised them that we would upload all the demo code we made for them to study and practise on :)</p>\n<p>Here&#x2019;s the main repository:<br><a href=\"https://github.com/kaseyhackspace/c_demo\" target=\"_blank\" rel=\"noopener\">https://github.com/kaseyhackspace/c_demo</a></p>\n<p>If you don&#x2019;t know how to use <code>git</code> and github.com, don&#x2019;t worry, I&#x2019;ll make a post about that one of these days when I get the time!</p>\n<p>In the meantime, here are links per chapter/topic so that you guys don&#x2019;t have to learn git right away :). Once you are redirected to the branch, go ahead and click on <code>main.c</code> in order to view the source code for that topic.</p>\n<ol>\n<li><a href=\"https://github.com/kaseyhackspace/c_demo/tree/1_cmd_line_arguments\" target=\"_blank\" rel=\"noopener\">Command line arguments</a></li>\n<li><a href=\"https://github.com/kaseyhackspace/c_demo/tree/2_input_output\" target=\"_blank\" rel=\"noopener\">Input Output</a></li>\n<li><a href=\"https://github.com/kaseyhackspace/c_demo/tree/3_operators\" target=\"_blank\" rel=\"noopener\">Arithmetic and Conditional Operators</a></li>\n<li><a href=\"https://github.com/kaseyhackspace/c_demo/tree/4_conditional\" target=\"_blank\" rel=\"noopener\">Conditional Statements</a></li>\n<li><a href=\"https://github.com/kaseyhackspace/c_demo/tree/5_switch\" target=\"_blank\" rel=\"noopener\">The Switch Statement</a></li>\n<li><a href=\"https://github.com/kaseyhackspace/c_demo/tree/6_loops\" target=\"_blank\" rel=\"noopener\">Loops</a></li>\n<li><a href=\"https://github.com/kaseyhackspace/c_demo/tree/7_functions\" target=\"_blank\" rel=\"noopener\">Functions</a></li>\n</ol>\n<p>There you have it, all the source code for last week&#x2019;s C programming workshop! Stay tuned as I&#x2019;ll be discussing each source code in detail in the days to come :)</p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{}},"excerpt":"","more":"<p>Last week we conducted a basic C Programming workshop in collaboration with <a href=\"https://www.facebook.com/MechatronicsSLU\" target=\"_blank\" rel=\"noopener\">Mechatronics SLU</a> at Saint Louis University, Baguio! It was a cool experience, and I really wanted to make sure that the students really understood the concepts we taught. After the session, we promised them that we would upload all the demo code we made for them to study and practise on :)</p>\n<p>Here’s the main repository:<br><a href=\"https://github.com/kaseyhackspace/c_demo\" target=\"_blank\" rel=\"noopener\">https://github.com/kaseyhackspace/c_demo</a></p>\n<p>If you don’t know how to use <code>git</code> and github.com, don’t worry, I’ll make a post about that one of these days when I get the time!</p>\n<p>In the meantime, here are links per chapter/topic so that you guys don’t have to learn git right away :). Once you are redirected to the branch, go ahead and click on <code>main.c</code> in order to view the source code for that topic.</p>\n<ol>\n<li><a href=\"https://github.com/kaseyhackspace/c_demo/tree/1_cmd_line_arguments\" target=\"_blank\" rel=\"noopener\">Command line arguments</a></li>\n<li><a href=\"https://github.com/kaseyhackspace/c_demo/tree/2_input_output\" target=\"_blank\" rel=\"noopener\">Input Output</a></li>\n<li><a href=\"https://github.com/kaseyhackspace/c_demo/tree/3_operators\" target=\"_blank\" rel=\"noopener\">Arithmetic and Conditional Operators</a></li>\n<li><a href=\"https://github.com/kaseyhackspace/c_demo/tree/4_conditional\" target=\"_blank\" rel=\"noopener\">Conditional Statements</a></li>\n<li><a href=\"https://github.com/kaseyhackspace/c_demo/tree/5_switch\" target=\"_blank\" rel=\"noopener\">The Switch Statement</a></li>\n<li><a href=\"https://github.com/kaseyhackspace/c_demo/tree/6_loops\" target=\"_blank\" rel=\"noopener\">Loops</a></li>\n<li><a href=\"https://github.com/kaseyhackspace/c_demo/tree/7_functions\" target=\"_blank\" rel=\"noopener\">Functions</a></li>\n</ol>\n<p>There you have it, all the source code for last week’s C programming workshop! Stay tuned as I’ll be discussing each source code in detail in the days to come :)</p>\n"},{"title":"Black screen after installing Nvidia drivers on Ubuntu 18.04 fix","date":"2018-10-13T14:26:34.000Z","_content":"\nI recently needed to reinstall Nvidia drivers as I performed a clean format to Ubuntu 18.04 after horribly crippling my system due to a failed `apt-get dist-upgrade`. Since I didn't have the time to be checking what the best driver version was for my graphics card, I went ahead and installed the drivers using the `sudo ubuntu-drivers autoinstall` command. Everything seemed ok, until I rebooted the system and was greeted with a black screen, and an unresponsive system.\n\nI did get to login to my system whenever I manually edited the grub configuration before booting up to change `quiet splash` to `nomodeset`. To make things more permanent, I did the following steps:\n\nEdit the `/etc/default/grub` file as root using `nano`:\n\n``` bash\n$ sudo nano /etc/default/grub\n```\n\nChange the line that has the `GRUB_CMDLINE_LINUX_DEFAULT` string from \n\n``` bash\nGRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash\"\n```\n\nto \n``` bash\nGRUB_CMDLINE_LINUX_DEFAULT=\"nomodeset\"\n```\n\nMy default grub configuration now looks like this:\n\n``` bash\n# If you change this file, run 'update-grub' afterwards to update\n# /boot/grub/grub.cfg.\n# For full documentation of the options in this file, see:\n#   info -f grub -n 'Simple configuration'\n\nGRUB_DEFAULT=0\nGRUB_TIMEOUT_STYLE=hidden\nGRUB_TIMEOUT=10\nGRUB_DISTRIBUTOR=`lsb_release -i -s 2> /dev/null || echo Debian`\nGRUB_CMDLINE_LINUX_DEFAULT=\"nomodeset\"\nGRUB_CMDLINE_LINUX=\"\"\n\n# Uncomment to enable BadRAM filtering, modify to suit your needs\n# This works with Linux (no patch required) and with any kernel that obtains\n# the memory map information from GRUB (GNU Mach, kernel of FreeBSD ...)\n#GRUB_BADRAM=\"0x01234567,0xfefefefe,0x89abcdef,0xefefefef\"\n\n# Uncomment to disable graphical terminal (grub-pc only)\n#GRUB_TERMINAL=console\n\n# The resolution used on graphical terminal\n# note that you can use only modes which your graphic card supports via VBE\n# you can see them in real GRUB with the command `vbeinfo'\n#GRUB_GFXMODE=640x480\n\n# Uncomment if you don't want GRUB to pass \"root=UUID=xxx\" parameter to Linux\n#GRUB_DISABLE_LINUX_UUID=true\n\n# Uncomment to disable generation of recovery mode menu entries\n#GRUB_DISABLE_RECOVERY=\"true\"\n\n# Uncomment to get a beep at grub start\n#GRUB_INIT_TUNE=\"480 440 1\"\n```\n\nAfter saving that file (using `nano`, do a `Ctl+X` then press `Y`), go ahead and regenerate your grub file using the command\n\n``` bash\nsudo update grub\n```\n\nOnce you reboot, you should have the Ubuntu recommended stable Nvidia drivers working! I know it works because my Google Earth Pro now renders the globe perfectly and the system is now smooth with its transitions :)\n\nI hope this post helps a lot of people out, and see you guys in my next post!","source":"_posts/Black-screen-after-installing-Nvidia-drivers-on-Ubuntu-18-04-fix.md","raw":"---\ntitle: Black screen after installing Nvidia drivers on Ubuntu 18.04 fix\ndate: 2018-10-13 22:26:34\ntags: \n    - nvidia\n    - drivers\n    - ubuntu\n    - '18.04'\n    - grub\n---\n\nI recently needed to reinstall Nvidia drivers as I performed a clean format to Ubuntu 18.04 after horribly crippling my system due to a failed `apt-get dist-upgrade`. Since I didn't have the time to be checking what the best driver version was for my graphics card, I went ahead and installed the drivers using the `sudo ubuntu-drivers autoinstall` command. Everything seemed ok, until I rebooted the system and was greeted with a black screen, and an unresponsive system.\n\nI did get to login to my system whenever I manually edited the grub configuration before booting up to change `quiet splash` to `nomodeset`. To make things more permanent, I did the following steps:\n\nEdit the `/etc/default/grub` file as root using `nano`:\n\n``` bash\n$ sudo nano /etc/default/grub\n```\n\nChange the line that has the `GRUB_CMDLINE_LINUX_DEFAULT` string from \n\n``` bash\nGRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash\"\n```\n\nto \n``` bash\nGRUB_CMDLINE_LINUX_DEFAULT=\"nomodeset\"\n```\n\nMy default grub configuration now looks like this:\n\n``` bash\n# If you change this file, run 'update-grub' afterwards to update\n# /boot/grub/grub.cfg.\n# For full documentation of the options in this file, see:\n#   info -f grub -n 'Simple configuration'\n\nGRUB_DEFAULT=0\nGRUB_TIMEOUT_STYLE=hidden\nGRUB_TIMEOUT=10\nGRUB_DISTRIBUTOR=`lsb_release -i -s 2> /dev/null || echo Debian`\nGRUB_CMDLINE_LINUX_DEFAULT=\"nomodeset\"\nGRUB_CMDLINE_LINUX=\"\"\n\n# Uncomment to enable BadRAM filtering, modify to suit your needs\n# This works with Linux (no patch required) and with any kernel that obtains\n# the memory map information from GRUB (GNU Mach, kernel of FreeBSD ...)\n#GRUB_BADRAM=\"0x01234567,0xfefefefe,0x89abcdef,0xefefefef\"\n\n# Uncomment to disable graphical terminal (grub-pc only)\n#GRUB_TERMINAL=console\n\n# The resolution used on graphical terminal\n# note that you can use only modes which your graphic card supports via VBE\n# you can see them in real GRUB with the command `vbeinfo'\n#GRUB_GFXMODE=640x480\n\n# Uncomment if you don't want GRUB to pass \"root=UUID=xxx\" parameter to Linux\n#GRUB_DISABLE_LINUX_UUID=true\n\n# Uncomment to disable generation of recovery mode menu entries\n#GRUB_DISABLE_RECOVERY=\"true\"\n\n# Uncomment to get a beep at grub start\n#GRUB_INIT_TUNE=\"480 440 1\"\n```\n\nAfter saving that file (using `nano`, do a `Ctl+X` then press `Y`), go ahead and regenerate your grub file using the command\n\n``` bash\nsudo update grub\n```\n\nOnce you reboot, you should have the Ubuntu recommended stable Nvidia drivers working! I know it works because my Google Earth Pro now renders the globe perfectly and the system is now smooth with its transitions :)\n\nI hope this post helps a lot of people out, and see you guys in my next post!","slug":"Black-screen-after-installing-Nvidia-drivers-on-Ubuntu-18-04-fix","published":1,"updated":"2018-12-01T06:39:52.814Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxg5gmdl00020lfo9jao9gw1","content":"<p>I recently needed to reinstall Nvidia drivers as I performed a clean format to Ubuntu 18.04 after horribly crippling my system due to a failed <code>apt-get dist-upgrade</code>. Since I didn&#x2019;t have the time to be checking what the best driver version was for my graphics card, I went ahead and installed the drivers using the <code>sudo ubuntu-drivers autoinstall</code> command. Everything seemed ok, until I rebooted the system and was greeted with a black screen, and an unresponsive system.</p>\n<p>I did get to login to my system whenever I manually edited the grub configuration before booting up to change <code>quiet splash</code> to <code>nomodeset</code>. To make things more permanent, I did the following steps:</p>\n<p>Edit the <code>/etc/default/grub</code> file as root using <code>nano</code>:</p>\n<figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo nano /etc/default/grub</span><br></pre></td></tr></tbody></table></figure>\n<p>Change the line that has the <code>GRUB_CMDLINE_LINUX_DEFAULT</code> string from </p>\n<figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GRUB_CMDLINE_LINUX_DEFAULT=<span class=\"string\">&quot;quiet splash&quot;</span></span><br></pre></td></tr></tbody></table></figure>\n<p>to<br></p><figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GRUB_CMDLINE_LINUX_DEFAULT=<span class=\"string\">&quot;nomodeset&quot;</span></span><br></pre></td></tr></tbody></table></figure><p></p>\n<p>My default grub configuration now looks like this:</p>\n<figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># If you change this file, run &apos;update-grub&apos; afterwards to update</span></span><br><span class=\"line\"><span class=\"comment\"># /boot/grub/grub.cfg.</span></span><br><span class=\"line\"><span class=\"comment\"># For full documentation of the options in this file, see:</span></span><br><span class=\"line\"><span class=\"comment\">#   info -f grub -n &apos;Simple configuration&apos;</span></span><br><span class=\"line\"></span><br><span class=\"line\">GRUB_DEFAULT=0</span><br><span class=\"line\">GRUB_TIMEOUT_STYLE=hidden</span><br><span class=\"line\">GRUB_TIMEOUT=10</span><br><span class=\"line\">GRUB_DISTRIBUTOR=`lsb_release -i -s 2&gt; /dev/null || <span class=\"built_in\">echo</span> Debian`</span><br><span class=\"line\">GRUB_CMDLINE_LINUX_DEFAULT=<span class=\"string\">&quot;nomodeset&quot;</span></span><br><span class=\"line\">GRUB_CMDLINE_LINUX=<span class=\"string\">&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Uncomment to enable BadRAM filtering, modify to suit your needs</span></span><br><span class=\"line\"><span class=\"comment\"># This works with Linux (no patch required) and with any kernel that obtains</span></span><br><span class=\"line\"><span class=\"comment\"># the memory map information from GRUB (GNU Mach, kernel of FreeBSD ...)</span></span><br><span class=\"line\"><span class=\"comment\">#GRUB_BADRAM=&quot;0x01234567,0xfefefefe,0x89abcdef,0xefefefef&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Uncomment to disable graphical terminal (grub-pc only)</span></span><br><span class=\"line\"><span class=\"comment\">#GRUB_TERMINAL=console</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># The resolution used on graphical terminal</span></span><br><span class=\"line\"><span class=\"comment\"># note that you can use only modes which your graphic card supports via VBE</span></span><br><span class=\"line\"><span class=\"comment\"># you can see them in real GRUB with the command `vbeinfo&apos;</span></span><br><span class=\"line\"><span class=\"comment\">#GRUB_GFXMODE=640x480</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Uncomment if you don&apos;t want GRUB to pass &quot;root=UUID=xxx&quot; parameter to Linux</span></span><br><span class=\"line\"><span class=\"comment\">#GRUB_DISABLE_LINUX_UUID=true</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Uncomment to disable generation of recovery mode menu entries</span></span><br><span class=\"line\"><span class=\"comment\">#GRUB_DISABLE_RECOVERY=&quot;true&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Uncomment to get a beep at grub start</span></span><br><span class=\"line\"><span class=\"comment\">#GRUB_INIT_TUNE=&quot;480 440 1&quot;</span></span><br></pre></td></tr></tbody></table></figure>\n<p>After saving that file (using <code>nano</code>, do a <code>Ctl+X</code> then press <code>Y</code>), go ahead and regenerate your grub file using the command</p>\n<figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo update grub</span><br></pre></td></tr></tbody></table></figure>\n<p>Once you reboot, you should have the Ubuntu recommended stable Nvidia drivers working! I know it works because my Google Earth Pro now renders the globe perfectly and the system is now smooth with its transitions :)</p>\n<p>I hope this post helps a lot of people out, and see you guys in my next post!</p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{}},"excerpt":"","more":"<p>I recently needed to reinstall Nvidia drivers as I performed a clean format to Ubuntu 18.04 after horribly crippling my system due to a failed <code>apt-get dist-upgrade</code>. Since I didn’t have the time to be checking what the best driver version was for my graphics card, I went ahead and installed the drivers using the <code>sudo ubuntu-drivers autoinstall</code> command. Everything seemed ok, until I rebooted the system and was greeted with a black screen, and an unresponsive system.</p>\n<p>I did get to login to my system whenever I manually edited the grub configuration before booting up to change <code>quiet splash</code> to <code>nomodeset</code>. To make things more permanent, I did the following steps:</p>\n<p>Edit the <code>/etc/default/grub</code> file as root using <code>nano</code>:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ sudo nano /etc/default/grub</span><br></pre></td></tr></table></figure>\n<p>Change the line that has the <code>GRUB_CMDLINE_LINUX_DEFAULT</code> string from </p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GRUB_CMDLINE_LINUX_DEFAULT=<span class=\"string\">\"quiet splash\"</span></span><br></pre></td></tr></table></figure>\n<p>to<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GRUB_CMDLINE_LINUX_DEFAULT=<span class=\"string\">\"nomodeset\"</span></span><br></pre></td></tr></table></figure></p>\n<p>My default grub configuration now looks like this:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># If you change this file, run 'update-grub' afterwards to update</span></span><br><span class=\"line\"><span class=\"comment\"># /boot/grub/grub.cfg.</span></span><br><span class=\"line\"><span class=\"comment\"># For full documentation of the options in this file, see:</span></span><br><span class=\"line\"><span class=\"comment\">#   info -f grub -n 'Simple configuration'</span></span><br><span class=\"line\"></span><br><span class=\"line\">GRUB_DEFAULT=0</span><br><span class=\"line\">GRUB_TIMEOUT_STYLE=hidden</span><br><span class=\"line\">GRUB_TIMEOUT=10</span><br><span class=\"line\">GRUB_DISTRIBUTOR=`lsb_release -i -s 2&gt; /dev/null || <span class=\"built_in\">echo</span> Debian`</span><br><span class=\"line\">GRUB_CMDLINE_LINUX_DEFAULT=<span class=\"string\">\"nomodeset\"</span></span><br><span class=\"line\">GRUB_CMDLINE_LINUX=<span class=\"string\">\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Uncomment to enable BadRAM filtering, modify to suit your needs</span></span><br><span class=\"line\"><span class=\"comment\"># This works with Linux (no patch required) and with any kernel that obtains</span></span><br><span class=\"line\"><span class=\"comment\"># the memory map information from GRUB (GNU Mach, kernel of FreeBSD ...)</span></span><br><span class=\"line\"><span class=\"comment\">#GRUB_BADRAM=\"0x01234567,0xfefefefe,0x89abcdef,0xefefefef\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Uncomment to disable graphical terminal (grub-pc only)</span></span><br><span class=\"line\"><span class=\"comment\">#GRUB_TERMINAL=console</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># The resolution used on graphical terminal</span></span><br><span class=\"line\"><span class=\"comment\"># note that you can use only modes which your graphic card supports via VBE</span></span><br><span class=\"line\"><span class=\"comment\"># you can see them in real GRUB with the command `vbeinfo'</span></span><br><span class=\"line\"><span class=\"comment\">#GRUB_GFXMODE=640x480</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Uncomment if you don't want GRUB to pass \"root=UUID=xxx\" parameter to Linux</span></span><br><span class=\"line\"><span class=\"comment\">#GRUB_DISABLE_LINUX_UUID=true</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Uncomment to disable generation of recovery mode menu entries</span></span><br><span class=\"line\"><span class=\"comment\">#GRUB_DISABLE_RECOVERY=\"true\"</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Uncomment to get a beep at grub start</span></span><br><span class=\"line\"><span class=\"comment\">#GRUB_INIT_TUNE=\"480 440 1\"</span></span><br></pre></td></tr></table></figure>\n<p>After saving that file (using <code>nano</code>, do a <code>Ctl+X</code> then press <code>Y</code>), go ahead and regenerate your grub file using the command</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo update grub</span><br></pre></td></tr></table></figure>\n<p>Once you reboot, you should have the Ubuntu recommended stable Nvidia drivers working! I know it works because my Google Earth Pro now renders the globe perfectly and the system is now smooth with its transitions :)</p>\n<p>I hope this post helps a lot of people out, and see you guys in my next post!</p>\n"},{"title":"Interpreting Marcus Aurelius 2","date":"2019-06-25T01:57:47.000Z","_content":"> Whatever it is, this being of mine is made up of flesh, breath, and directing mind. Now the flesh you should disdain - blood, bones, a mere fabric and network of nerves, veins, and arteries. Consider too what breath is: wind - and not even a constant, but all the time being disgorged and sucked in again. That leaves the third part, the directing mind. Quit your books - no more hankering: this is not your gift. No, think like this, as if you were on the point of death: \"you are old; don't then let this directing mind of yours be enslaved any longer - no more jerking to the strings of selfish impulse, no more disquiet at your present or suspicion of your future fate.\"\n> -- <cite>Marcus Aurelius</cite>\n\nMarcus Aurelius in this passage contemplates what elements we are composed of and how important each one is. He divides these elements into three distinctions. The first is our body, which he states is just a network of nerves, veins and arteries covered by skin (fabric in his words). Next is our breath, which he states is not a permanent part of us, since we continually breathe it. With each breath we convert oxygen to carbon dioxide, so this is very true as each breath is different than the last. The last, and most importantly is the directing mind that governs us. Rather than simplifying the mind physically as he did with the body and the breath, Marcus says \"Quit your books - no more hankering: this is not your gift.\" This shows that he puts the directing mind of utmost importance in our very being. What is also very interesting is that Marcus urges us to stop overthinking when he says \"Quit your books\" and \"no more disquiet at your present or suspicion of your future fate.\" Too much overthinking can paralyse us, wasting the limited time we have in this life. In contrast, Marcus also tells us to be more calculating, by telling us to stop \"jerking to the strings of self impulse.\" To react based on impulse is of the body which is ever fleating, and not of the all important directing mind.\n\nThis passage really hits home for me as a few months ago, I often found myself both in two sides of the spectrum. When working, I take too much time trying to make sure everything is perfect rather than acting based on simple yet effective solutions to problems. When encountering unforeseen situations, I used to act rashly and regret my actions later. When I started acting before getting in too deep into a thought and also controlling my knee jerk reactions, I found myself in a much more better place where I got a lot more done and avoided a lot of situations that could've started serious conflicts.","source":"_posts/Interpreting-Marcus-Aurelius-2.md","raw":"---\ntitle: Interpreting Marcus Aurelius 2\ndate: 2019-06-25 09:57:47\ntags:\n    - marcus aurelius\n    - stoicism\n    - stoic\n---\n> Whatever it is, this being of mine is made up of flesh, breath, and directing mind. Now the flesh you should disdain - blood, bones, a mere fabric and network of nerves, veins, and arteries. Consider too what breath is: wind - and not even a constant, but all the time being disgorged and sucked in again. That leaves the third part, the directing mind. Quit your books - no more hankering: this is not your gift. No, think like this, as if you were on the point of death: \"you are old; don't then let this directing mind of yours be enslaved any longer - no more jerking to the strings of selfish impulse, no more disquiet at your present or suspicion of your future fate.\"\n> -- <cite>Marcus Aurelius</cite>\n\nMarcus Aurelius in this passage contemplates what elements we are composed of and how important each one is. He divides these elements into three distinctions. The first is our body, which he states is just a network of nerves, veins and arteries covered by skin (fabric in his words). Next is our breath, which he states is not a permanent part of us, since we continually breathe it. With each breath we convert oxygen to carbon dioxide, so this is very true as each breath is different than the last. The last, and most importantly is the directing mind that governs us. Rather than simplifying the mind physically as he did with the body and the breath, Marcus says \"Quit your books - no more hankering: this is not your gift.\" This shows that he puts the directing mind of utmost importance in our very being. What is also very interesting is that Marcus urges us to stop overthinking when he says \"Quit your books\" and \"no more disquiet at your present or suspicion of your future fate.\" Too much overthinking can paralyse us, wasting the limited time we have in this life. In contrast, Marcus also tells us to be more calculating, by telling us to stop \"jerking to the strings of self impulse.\" To react based on impulse is of the body which is ever fleating, and not of the all important directing mind.\n\nThis passage really hits home for me as a few months ago, I often found myself both in two sides of the spectrum. When working, I take too much time trying to make sure everything is perfect rather than acting based on simple yet effective solutions to problems. When encountering unforeseen situations, I used to act rashly and regret my actions later. When I started acting before getting in too deep into a thought and also controlling my knee jerk reactions, I found myself in a much more better place where I got a lot more done and avoided a lot of situations that could've started serious conflicts.","slug":"Interpreting-Marcus-Aurelius-2","published":1,"updated":"2019-06-28T13:45:17.032Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxg5gmdp00050lfoems0zyjn","content":"<blockquote>\n<p>Whatever it is, this being of mine is made up of flesh, breath, and directing mind. Now the flesh you should disdain - blood, bones, a mere fabric and network of nerves, veins, and arteries. Consider too what breath is: wind - and not even a constant, but all the time being disgorged and sucked in again. That leaves the third part, the directing mind. Quit your books - no more hankering: this is not your gift. No, think like this, as if you were on the point of death: &#x201C;you are old; don&#x2019;t then let this directing mind of yours be enslaved any longer - no more jerking to the strings of selfish impulse, no more disquiet at your present or suspicion of your future fate.&#x201D;<br>&#x2013; <cite>Marcus Aurelius</cite></p>\n</blockquote>\n<p>Marcus Aurelius in this passage contemplates what elements we are composed of and how important each one is. He divides these elements into three distinctions. The first is our body, which he states is just a network of nerves, veins and arteries covered by skin (fabric in his words). Next is our breath, which he states is not a permanent part of us, since we continually breathe it. With each breath we convert oxygen to carbon dioxide, so this is very true as each breath is different than the last. The last, and most importantly is the directing mind that governs us. Rather than simplifying the mind physically as he did with the body and the breath, Marcus says &#x201C;Quit your books - no more hankering: this is not your gift.&#x201D; This shows that he puts the directing mind of utmost importance in our very being. What is also very interesting is that Marcus urges us to stop overthinking when he says &#x201C;Quit your books&#x201D; and &#x201C;no more disquiet at your present or suspicion of your future fate.&#x201D; Too much overthinking can paralyse us, wasting the limited time we have in this life. In contrast, Marcus also tells us to be more calculating, by telling us to stop &#x201C;jerking to the strings of self impulse.&#x201D; To react based on impulse is of the body which is ever fleating, and not of the all important directing mind.</p>\n<p>This passage really hits home for me as a few months ago, I often found myself both in two sides of the spectrum. When working, I take too much time trying to make sure everything is perfect rather than acting based on simple yet effective solutions to problems. When encountering unforeseen situations, I used to act rashly and regret my actions later. When I started acting before getting in too deep into a thought and also controlling my knee jerk reactions, I found myself in a much more better place where I got a lot more done and avoided a lot of situations that could&#x2019;ve started serious conflicts.</p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>Whatever it is, this being of mine is made up of flesh, breath, and directing mind. Now the flesh you should disdain - blood, bones, a mere fabric and network of nerves, veins, and arteries. Consider too what breath is: wind - and not even a constant, but all the time being disgorged and sucked in again. That leaves the third part, the directing mind. Quit your books - no more hankering: this is not your gift. No, think like this, as if you were on the point of death: “you are old; don’t then let this directing mind of yours be enslaved any longer - no more jerking to the strings of selfish impulse, no more disquiet at your present or suspicion of your future fate.”<br>– <cite>Marcus Aurelius</cite></p>\n</blockquote>\n<p>Marcus Aurelius in this passage contemplates what elements we are composed of and how important each one is. He divides these elements into three distinctions. The first is our body, which he states is just a network of nerves, veins and arteries covered by skin (fabric in his words). Next is our breath, which he states is not a permanent part of us, since we continually breathe it. With each breath we convert oxygen to carbon dioxide, so this is very true as each breath is different than the last. The last, and most importantly is the directing mind that governs us. Rather than simplifying the mind physically as he did with the body and the breath, Marcus says “Quit your books - no more hankering: this is not your gift.” This shows that he puts the directing mind of utmost importance in our very being. What is also very interesting is that Marcus urges us to stop overthinking when he says “Quit your books” and “no more disquiet at your present or suspicion of your future fate.” Too much overthinking can paralyse us, wasting the limited time we have in this life. In contrast, Marcus also tells us to be more calculating, by telling us to stop “jerking to the strings of self impulse.” To react based on impulse is of the body which is ever fleating, and not of the all important directing mind.</p>\n<p>This passage really hits home for me as a few months ago, I often found myself both in two sides of the spectrum. When working, I take too much time trying to make sure everything is perfect rather than acting based on simple yet effective solutions to problems. When encountering unforeseen situations, I used to act rashly and regret my actions later. When I started acting before getting in too deep into a thought and also controlling my knee jerk reactions, I found myself in a much more better place where I got a lot more done and avoided a lot of situations that could’ve started serious conflicts.</p>\n"},{"title":"Interpreting Marcus Aurelius 3","date":"2019-06-28T13:37:57.000Z","_content":"","source":"_posts/Interpreting-Marcus-Aurelius-3.md","raw":"---\ntitle: Interpreting Marcus Aurelius 3\ndate: 2019-06-28 21:37:57\ntags:\n---\n","slug":"Interpreting-Marcus-Aurelius-3","published":1,"updated":"2019-06-28T13:37:57.582Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxg5gmdq00060lfon83y7byj","content":"<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{}},"excerpt":"","more":""},{"title":"Interpreting Marcus Aurelius 1","date":"2019-06-23T22:48:02.000Z","_content":"> Say to yourself the first thing in the morning: today I shall meet people who are meddling, ungrateful, aggressive, treacherous, malicious, unsocial. All this has afflicted them through their ignorance of true good and evil. But I have seen that the nature of good is what is right, and the nature of evil is what is wrong; and I have reflected that the nature of the offender himself is akin to my own - not a kinship of blood or seed, but a sharing in the same mind, the same fragment of divinity. Therefore I cannot be harmed by any of them, as none will infect me with their wrong. Nor can I be angry with my kinsman or hate him. We were born for cooperation, like feet, like hands, like eyelids, like rows of upper and lower teeth. So to work in opposition to one another is against nature: and anger or rejection is opposition.\n> -- <cite>Marcus Aurelius</cite>\n\nIn our day to day lives, it is not uncommon to encounter people who are very meddling, ungrateful, aggresive, treacherous, malicious, unsocial, or any other trait that we consider bad. The important thing to note is that we must realize that there are things that we cannot control, and things that we can control. This is referred to in traditional stoicism as the Dichotomy of Control. How other people act towards us is something that is not in our control. What is in our total control is how we react to these types of people. Rather than showing anger, Marcus Aurelius tells us to prepare ourselves each day by accepting that we know what is right from wrong, and what is good from evil. We are aware of such and they are not, so we should not act rashly in anger and retaliate. We are of the same nature and fragment of divinity, so acting in opposition to them is against that very nature. Allowing yourself each day to remember this will give you a certain script of how to deal with wrongdoers, if ever you may encounter them each day.","source":"_posts/Interpreting-Marcus-Aurelius-1.md","raw":"---\ntitle: Interpreting Marcus Aurelius 1\ndate: 2019-06-24 06:48:02\ntags:\n    - marcus aurelius\n    - stoicism\n    - stoic\n---\n> Say to yourself the first thing in the morning: today I shall meet people who are meddling, ungrateful, aggressive, treacherous, malicious, unsocial. All this has afflicted them through their ignorance of true good and evil. But I have seen that the nature of good is what is right, and the nature of evil is what is wrong; and I have reflected that the nature of the offender himself is akin to my own - not a kinship of blood or seed, but a sharing in the same mind, the same fragment of divinity. Therefore I cannot be harmed by any of them, as none will infect me with their wrong. Nor can I be angry with my kinsman or hate him. We were born for cooperation, like feet, like hands, like eyelids, like rows of upper and lower teeth. So to work in opposition to one another is against nature: and anger or rejection is opposition.\n> -- <cite>Marcus Aurelius</cite>\n\nIn our day to day lives, it is not uncommon to encounter people who are very meddling, ungrateful, aggresive, treacherous, malicious, unsocial, or any other trait that we consider bad. The important thing to note is that we must realize that there are things that we cannot control, and things that we can control. This is referred to in traditional stoicism as the Dichotomy of Control. How other people act towards us is something that is not in our control. What is in our total control is how we react to these types of people. Rather than showing anger, Marcus Aurelius tells us to prepare ourselves each day by accepting that we know what is right from wrong, and what is good from evil. We are aware of such and they are not, so we should not act rashly in anger and retaliate. We are of the same nature and fragment of divinity, so acting in opposition to them is against that very nature. Allowing yourself each day to remember this will give you a certain script of how to deal with wrongdoers, if ever you may encounter them each day.","slug":"Interpreting-Marcus-Aurelius-1","published":1,"updated":"2019-06-24T02:27:15.574Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxg5gmds00070lfoihm4pluj","content":"<blockquote>\n<p>Say to yourself the first thing in the morning: today I shall meet people who are meddling, ungrateful, aggressive, treacherous, malicious, unsocial. All this has afflicted them through their ignorance of true good and evil. But I have seen that the nature of good is what is right, and the nature of evil is what is wrong; and I have reflected that the nature of the offender himself is akin to my own - not a kinship of blood or seed, but a sharing in the same mind, the same fragment of divinity. Therefore I cannot be harmed by any of them, as none will infect me with their wrong. Nor can I be angry with my kinsman or hate him. We were born for cooperation, like feet, like hands, like eyelids, like rows of upper and lower teeth. So to work in opposition to one another is against nature: and anger or rejection is opposition.<br>&#x2013; <cite>Marcus Aurelius</cite></p>\n</blockquote>\n<p>In our day to day lives, it is not uncommon to encounter people who are very meddling, ungrateful, aggresive, treacherous, malicious, unsocial, or any other trait that we consider bad. The important thing to note is that we must realize that there are things that we cannot control, and things that we can control. This is referred to in traditional stoicism as the Dichotomy of Control. How other people act towards us is something that is not in our control. What is in our total control is how we react to these types of people. Rather than showing anger, Marcus Aurelius tells us to prepare ourselves each day by accepting that we know what is right from wrong, and what is good from evil. We are aware of such and they are not, so we should not act rashly in anger and retaliate. We are of the same nature and fragment of divinity, so acting in opposition to them is against that very nature. Allowing yourself each day to remember this will give you a certain script of how to deal with wrongdoers, if ever you may encounter them each day.</p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>Say to yourself the first thing in the morning: today I shall meet people who are meddling, ungrateful, aggressive, treacherous, malicious, unsocial. All this has afflicted them through their ignorance of true good and evil. But I have seen that the nature of good is what is right, and the nature of evil is what is wrong; and I have reflected that the nature of the offender himself is akin to my own - not a kinship of blood or seed, but a sharing in the same mind, the same fragment of divinity. Therefore I cannot be harmed by any of them, as none will infect me with their wrong. Nor can I be angry with my kinsman or hate him. We were born for cooperation, like feet, like hands, like eyelids, like rows of upper and lower teeth. So to work in opposition to one another is against nature: and anger or rejection is opposition.<br>– <cite>Marcus Aurelius</cite></p>\n</blockquote>\n<p>In our day to day lives, it is not uncommon to encounter people who are very meddling, ungrateful, aggresive, treacherous, malicious, unsocial, or any other trait that we consider bad. The important thing to note is that we must realize that there are things that we cannot control, and things that we can control. This is referred to in traditional stoicism as the Dichotomy of Control. How other people act towards us is something that is not in our control. What is in our total control is how we react to these types of people. Rather than showing anger, Marcus Aurelius tells us to prepare ourselves each day by accepting that we know what is right from wrong, and what is good from evil. We are aware of such and they are not, so we should not act rashly in anger and retaliate. We are of the same nature and fragment of divinity, so acting in opposition to them is against that very nature. Allowing yourself each day to remember this will give you a certain script of how to deal with wrongdoers, if ever you may encounter them each day.</p>\n"},{"title":"Project Compozen: Build Log 0","date":"2018-12-03T11:10:25.000Z","_content":"\nI've always been a `Python` guy and I've been putting off learning `Node.js` for so long. I didn't even know the fundamentals of `Node.js`. Part of the reason was I couldn't get it off my head that `javascript` was primarily used for client side browser scripting. But the thing is, `Node.js` enabled `javascript` to run on the server-side. But what really reeled me in is that `Node.js` was built using Chrome's V8 Javascript engine, which makes it really, really, fast. I love fast programs, and if you've known me in college, I was obsessed with making my programs fast and efficient. Hence my love for `C`, `C++`, assembly languages, parallel, distributed and GPU computing :)\n\nMy first opensource project in `Node.js` will be `Compozen`! What is `Compozen`? `Compozen` will be a CLI tool for generating and managing `docker-compose` YAML files! These files are used by `docker-compose` a tool used `Docker` to spin up of micro-services that work together in order to make modern applications using Docker work. \n\nWhy do I want to create `Compozen`? Well, I want to introduce it to the Continous Integration and Continous Delivery pipeline I'm building for the awesome clients I work for and I want to use it for my future projects. Hopefully, you guys can use it too when the time comes for you to build your own awesome ideas {% github_emoji grin %}\n\nHere's the repository for it : https://github.com/kaseyhackspace/compozen\n\nI'll keep you guys updated about `Compozen` using these Build Logs reguarly to keep myself in check.","source":"_posts/Project-Compozen-Build-Log-0.md","raw":"---\ntitle: 'Project Compozen: Build Log 0'\ndate: 2018-12-03 19:10:25\ntags:\n    - node.js\n    - cli\n    - opensource\n    - docker\n    - docker-compose\n---\n\nI've always been a `Python` guy and I've been putting off learning `Node.js` for so long. I didn't even know the fundamentals of `Node.js`. Part of the reason was I couldn't get it off my head that `javascript` was primarily used for client side browser scripting. But the thing is, `Node.js` enabled `javascript` to run on the server-side. But what really reeled me in is that `Node.js` was built using Chrome's V8 Javascript engine, which makes it really, really, fast. I love fast programs, and if you've known me in college, I was obsessed with making my programs fast and efficient. Hence my love for `C`, `C++`, assembly languages, parallel, distributed and GPU computing :)\n\nMy first opensource project in `Node.js` will be `Compozen`! What is `Compozen`? `Compozen` will be a CLI tool for generating and managing `docker-compose` YAML files! These files are used by `docker-compose` a tool used `Docker` to spin up of micro-services that work together in order to make modern applications using Docker work. \n\nWhy do I want to create `Compozen`? Well, I want to introduce it to the Continous Integration and Continous Delivery pipeline I'm building for the awesome clients I work for and I want to use it for my future projects. Hopefully, you guys can use it too when the time comes for you to build your own awesome ideas {% github_emoji grin %}\n\nHere's the repository for it : https://github.com/kaseyhackspace/compozen\n\nI'll keep you guys updated about `Compozen` using these Build Logs reguarly to keep myself in check.","slug":"Project-Compozen-Build-Log-0","published":1,"updated":"2018-12-07T15:17:09.122Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxg5gmdt00090lfo1e3uy5g1","content":"<p>I&#x2019;ve always been a <code>Python</code> guy and I&#x2019;ve been putting off learning <code>Node.js</code> for so long. I didn&#x2019;t even know the fundamentals of <code>Node.js</code>. Part of the reason was I couldn&#x2019;t get it off my head that <code>javascript</code> was primarily used for client side browser scripting. But the thing is, <code>Node.js</code> enabled <code>javascript</code> to run on the server-side. But what really reeled me in is that <code>Node.js</code> was built using Chrome&#x2019;s V8 Javascript engine, which makes it really, really, fast. I love fast programs, and if you&#x2019;ve known me in college, I was obsessed with making my programs fast and efficient. Hence my love for <code>C</code>, <code>C++</code>, assembly languages, parallel, distributed and GPU computing :)</p>\n<p>My first opensource project in <code>Node.js</code> will be <code>Compozen</code>! What is <code>Compozen</code>? <code>Compozen</code> will be a CLI tool for generating and managing <code>docker-compose</code> YAML files! These files are used by <code>docker-compose</code> a tool used <code>Docker</code> to spin up of micro-services that work together in order to make modern applications using Docker work. </p>\n<p>Why do I want to create <code>Compozen</code>? Well, I want to introduce it to the Continous Integration and Continous Delivery pipeline I&#x2019;m building for the awesome clients I work for and I want to use it for my future projects. Hopefully, you guys can use it too when the time comes for you to build your own awesome ideas <span class=\"github-emoji\" style=\"color: transparent;background:no-repeat url(https://assets-cdn.github.com/images/icons/emoji/unicode/1f601.png?v8) center/contain\" data-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f601.png?v8\">&#x1F601;</span></p>\n<p>Here&#x2019;s the repository for it : <a href=\"https://github.com/kaseyhackspace/compozen\" target=\"_blank\" rel=\"noopener\">https://github.com/kaseyhackspace/compozen</a></p>\n<p>I&#x2019;ll keep you guys updated about <code>Compozen</code> using these Build Logs reguarly to keep myself in check.</p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{}},"excerpt":"","more":"<p>I’ve always been a <code>Python</code> guy and I’ve been putting off learning <code>Node.js</code> for so long. I didn’t even know the fundamentals of <code>Node.js</code>. Part of the reason was I couldn’t get it off my head that <code>javascript</code> was primarily used for client side browser scripting. But the thing is, <code>Node.js</code> enabled <code>javascript</code> to run on the server-side. But what really reeled me in is that <code>Node.js</code> was built using Chrome’s V8 Javascript engine, which makes it really, really, fast. I love fast programs, and if you’ve known me in college, I was obsessed with making my programs fast and efficient. Hence my love for <code>C</code>, <code>C++</code>, assembly languages, parallel, distributed and GPU computing :)</p>\n<p>My first opensource project in <code>Node.js</code> will be <code>Compozen</code>! What is <code>Compozen</code>? <code>Compozen</code> will be a CLI tool for generating and managing <code>docker-compose</code> YAML files! These files are used by <code>docker-compose</code> a tool used <code>Docker</code> to spin up of micro-services that work together in order to make modern applications using Docker work. </p>\n<p>Why do I want to create <code>Compozen</code>? Well, I want to introduce it to the Continous Integration and Continous Delivery pipeline I’m building for the awesome clients I work for and I want to use it for my future projects. Hopefully, you guys can use it too when the time comes for you to build your own awesome ideas <span class=\"github-emoji\" style=\"color: transparent;background:no-repeat url(https://assets-cdn.github.com/images/icons/emoji/unicode/1f601.png?v8) center/contain\" data-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f601.png?v8\">&#x1f601;</span></p>\n<p>Here’s the repository for it : <a href=\"https://github.com/kaseyhackspace/compozen\" target=\"_blank\" rel=\"noopener\">https://github.com/kaseyhackspace/compozen</a></p>\n<p>I’ll keep you guys updated about <code>Compozen</code> using these Build Logs reguarly to keep myself in check.</p>\n"},{"title":"Hello World","date":"2018-09-27T13:00:00.000Z","_content":"**Hello World!** If you've done any programming, this sentence is all too familliar to you :). I'm Kasey, and this is the latest of my attempts to start a blog. I started a few along the years, but I keep on forgetting to add to it after the first post. Hopefully this time around, it takes a different turn and I end up posting regularly!\n\n## What will this blog be about?\n\nWell, this blog is going to be about a _LOT_ of stuff! I am thinking mainly about _Computer Science_ topics, but I'll also do posts about life, health, travel, and a lot of meaningful stuff! I'll be spending quality time writing these blogs to hopefully provide good insight to my readers :)\n\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ndate: 2018-09-27 21:00:00\n---\n**Hello World!** If you've done any programming, this sentence is all too familliar to you :). I'm Kasey, and this is the latest of my attempts to start a blog. I started a few along the years, but I keep on forgetting to add to it after the first post. Hopefully this time around, it takes a different turn and I end up posting regularly!\n\n## What will this blog be about?\n\nWell, this blog is going to be about a _LOT_ of stuff! I am thinking mainly about _Computer Science_ topics, but I'll also do posts about life, health, travel, and a lot of meaningful stuff! I'll be spending quality time writing these blogs to hopefully provide good insight to my readers :)\n\n","slug":"hello-world","published":1,"updated":"2018-12-01T06:39:52.814Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxg5gmdu000a0lfoq2o6bvwb","content":"<p><strong>Hello World!</strong> If you&#x2019;ve done any programming, this sentence is all too familliar to you :). I&#x2019;m Kasey, and this is the latest of my attempts to start a blog. I started a few along the years, but I keep on forgetting to add to it after the first post. Hopefully this time around, it takes a different turn and I end up posting regularly!</p>\n<h2 id=\"What-will-this-blog-be-about\"><a href=\"#What-will-this-blog-be-about\" class=\"headerlink\" title=\"What will this blog be about?\"></a>What will this blog be about?</h2><p>Well, this blog is going to be about a <em>LOT</em> of stuff! I am thinking mainly about <em>Computer Science</em> topics, but I&#x2019;ll also do posts about life, health, travel, and a lot of meaningful stuff! I&#x2019;ll be spending quality time writing these blogs to hopefully provide good insight to my readers :)</p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{}},"excerpt":"","more":"<p><strong>Hello World!</strong> If you’ve done any programming, this sentence is all too familliar to you :). I’m Kasey, and this is the latest of my attempts to start a blog. I started a few along the years, but I keep on forgetting to add to it after the first post. Hopefully this time around, it takes a different turn and I end up posting regularly!</p>\n<h2 id=\"What-will-this-blog-be-about\"><a href=\"#What-will-this-blog-be-about\" class=\"headerlink\" title=\"What will this blog be about?\"></a>What will this blog be about?</h2><p>Well, this blog is going to be about a <em>LOT</em> of stuff! I am thinking mainly about <em>Computer Science</em> topics, but I’ll also do posts about life, health, travel, and a lot of meaningful stuff! I’ll be spending quality time writing these blogs to hopefully provide good insight to my readers :)</p>\n"},{"title":"The one about N-Grams and Markov Chains","date":"2019-04-21T00:30:00.000Z","_content":"Today we are going to make a Markov Chain to generate text from a training dataset we are going to feed it. So how do we start? We start by first implementing n-grams. N-grams are a list of n words that share a common border in a given string. It may sound confusing, so it is better that I implement it first then show by example:\n\n\n```python\nclass NGram:\n\n    def ngram(self, text, n=2):\n        text_list = text.lower().split()\n        grams = [tuple(text_list[index:index+n]) for index in range(0, len(text_list) - (n-1))]\n        return grams\n\n    def bigram(self, text):\n        return self.ngram(text, n=2)\n\n    def trigram(self, text):\n        return self.ngram(text, n=3)\n```\n\nI defined a class called `NGram` that contains three functions. The first function `ngram` takes in a string variable `text` and an integer `n` to create an n-gram. By default, if no `n` is passed, then `n=2` and we get what's called a bigram (bi meaning 2). In line 4 I do some simple preprocessing of the text by converting everything to lowercase with the `lower` function and then splitting the string per word into a list with the `split` function (if you don't specify a separator in the `split` function, it splits the string by whitespace). n-grams are actually generated in line 5 where I generate a list of `tuples` using list indexing and list comprehension techniques.\n\n`bigram` and `trigram` functions are used to make n-grams where `n=2` and `n=3` respectively. As you can see, they use the `ngram` function under the hood. Lets have a few examples so you can see what the piece of code does on out sample text: \"Hello word, this is a sample text for consumption of the NGram class!\"\n\n\n```python\nsample = \"Hello word, this is a sample text for consumption of the NGram class!\"\nngram = NGram()\n# bi-gram example\nprint(ngram.bigram(sample))\n```\n\n    [('hello', 'word,'), ('word,', 'this'), ('this', 'is'), ('is', 'a'), ('a', 'sample'), ('sample', 'text'), ('text', 'for'), ('for', 'consumption'), ('consumption', 'of'), ('of', 'the'), ('the', 'ngram'), ('ngram', 'class!')]\n\n\n\n```python\n# tri-gram example\nprint(ngram.trigram(sample))\n```\n\n    [('hello', 'word,', 'this'), ('word,', 'this', 'is'), ('this', 'is', 'a'), ('is', 'a', 'sample'), ('a', 'sample', 'text'), ('sample', 'text', 'for'), ('text', 'for', 'consumption'), ('for', 'consumption', 'of'), ('consumption', 'of', 'the'), ('of', 'the', 'ngram'), ('the', 'ngram', 'class!')]\n\n\n\n```python\n# n-gram example where n=4\nprint(ngram.ngram(sample,n=4))\n```\n\n    [('hello', 'word,', 'this', 'is'), ('word,', 'this', 'is', 'a'), ('this', 'is', 'a', 'sample'), ('is', 'a', 'sample', 'text'), ('a', 'sample', 'text', 'for'), ('sample', 'text', 'for', 'consumption'), ('text', 'for', 'consumption', 'of'), ('for', 'consumption', 'of', 'the'), ('consumption', 'of', 'the', 'ngram'), ('of', 'the', 'ngram', 'class!')]\n\n\nI do hope the examples above made n-grams clearer than I could explain them :). Now that we've implemented our NGram class, we can start building our shiny Markov Chain! The markov chain class is a bit lengthy, so I chose to write docstrings under the function names instead so you have have a high level view of the class and dig in to investigate the code if you want.\n\n\n```python\nimport numpy as np\n\nclass MarkovChain:\n    \n    def __init__(self):\n        \"\"\"\n        Initialization creates empty dictionary\n        \"\"\"\n        self.model = {}\n    \n    def calculate_probabilities(self):\n        \"\"\"\n        per word in model, calculate the probability of next words following the current word\n        based on the bigram pair and how many times the bigram pair appears in the training dataset\n        \"\"\"\n        for key in self.model:\n            total_sum = np.sum(self.model[key]['counts'])\n            self.model[key]['probability'] = [x/total_sum for x in self.model[key]['counts']]\n            \n    def train_model(self,text):\n        \"\"\"\n        The train_model method first generates bigrams from the given text string. The method then\n        traverses each bigram and determines if the first word in the bigram already exists in the model.\n        The behavior of the method per bigram is determined with the following cases:\n        \n        If the first element has not been added yet(does not have a key in the dictionary), the first \n        element is added to the dictionary with the second element added in the `values` list with a \n        corresponding occurance count of 1.\n        \n        If the first element has already been added but the second element has not yet been added to the\n        `values` list, it appends the second element to the `values` list with a corresponding occurance count \n        of 1. \n        \n        If both the first and second elements of the bigram already exist, just increment the number of\n        occurances of the second element by 1.\n        \n        After all bigrams have been processed, the train_model method calls the calculate_probability method\n        to calculate the probability of choosing the next word randomly given the current work in the Markov\n        Chain.\n        \n        The final model would have the following structure:\n        \n        {\n            'word-1': {\n                values: [<list_of_probable_words>]\n                counts: [<number_of_occurences_per_word_in_values>]\n                probability: [<probability_of_occurence_per_word_in_values>]\n            },\n            'word-2': {\n                values: [<list_of_probable_words>]\n                counts: [<number_of_occurences_per_word_in_values>]\n                probability: [<probability_of_occurence_per_word_in_values>]\n            },\n            ...,\n            'word-n': {\n                values: [<list_of_probable_words>]\n                counts: [<number_of_occurences_per_word_in_values>]\n                probability: [<probability_of_occurence_per_word_in_values>]\n            }\n        }\n        \"\"\"\n        ngram = NGram()\n        \n        bigrams = ngram.bigram(text)\n        \n        for bigram in bigrams:\n            if bigram[0] not in self.model:\n                self.model[bigram[0]]= {\n                    'values': [bigram[1]],\n                    'counts': [1],\n                    'probability': []\n                }\n            else:\n                if bigram[1] not in self.model[bigram[0]]['values']:\n                    self.model[bigram[0]]['values'].append(bigram[1])\n                    self.model[bigram[0]]['counts'].append(1)\n                else:\n                    index = self.model[bigram[0]]['values'].index(bigram[1])\n                    self.model[bigram[0]]['counts'][index] = self.model[bigram[0]]['counts'][index] + 1\n            \n            self.calculate_probabilities()\n    \n    def generate_text(self, n):\n        \"\"\"\n        This method generates a random sentence of n words. It selects a random starting point from the model\n        then chooses a random hop to the next word based on the probability array of the word list n times.\n        \n        Note that it is possible to have a word that does not have next hops, we gracefully handle that with\n        a try except case that returns the sentence prematurely if ever that happens.\n        \"\"\"\n        sentence = ''\n        current_word = np.random.choice(list(self.model.keys()))\n        sentence += current_word\n        for index in range(n):\n            try:\n                current_word = np.random.choice(self.model[current_word]['values'],p=self.model[current_word]['probability'])\n                sentence += ' '+current_word\n            except:\n                return sentence   \n        return sentence\n    \n    def generate_sentence(self):\n        \"\"\"\n        This method generates a random sentence of . It selects a random starting point from the model\n        then chooses a random hop to the next word based on the probability array of the word list until\n        it encounters a \".\".\n        \n        Note that it is possible to have a word that does not have next hops, we gracefully handle that with\n        a try except case that returns the sentence prematurely if ever that happens.\n        \"\"\"\n        sentence = ''\n        current_word = np.random.choice(list(self.model.keys()))\n        sentence += current_word\n        while '.' not in current_word:\n            try:\n                current_word = np.random.choice(self.model[current_word]['values'],p=self.model[current_word]['probability'])\n                sentence += ' '+current_word\n            except:\n                return sentence   \n        return sentence\n    \n```\n\nNow that we have the Markov Chain class implemented, we can test out the code! For the training dataset, I chose wikipedia's summary of the [Avengers: infinity war](https://en.wikipedia.org/wiki/Avengers:_Infinity_War#Plot) movie.\n\n\n```python\n\ntraining_text = \"Having acquired the Power Stone, one of the six Infinity Stones, from the planet Xandar, Thanos and his lieutenants—Ebony Maw, Cull Obsidian, Proxima Midnight, and Corvus Glaive—intercept the spaceship carrying the surviving Asgardians. As they extract the Space Stone from the Tesseract, Thanos subdues Thor, overpowers Hulk, and kills Loki. Heimdall sends Hulk to Earth using the Bifröst before being killed. Thanos departs with his lieutenants and obliterates the ship. Hulk crash-lands at the Sanctum Sanctorum in New York City, reverting to Bruce Banner. He warns Stephen Strange and Wong about Thanos' plan to kill half of all life in the universe; in response, Strange recruits Tony Stark. Maw and Obsidian arrive to retrieve the Time Stone from Strange, drawing the attention of Peter Parker. Maw captures Strange, but fails to take the Time Stone due to an enchantment. Stark and Parker pursue Maw's spaceship, Banner contacts Steve Rogers, and Wong stays behind to guard the Sanctum. In Edinburgh, Midnight and Glaive ambush Wanda Maximoff and Vision in order to retrieve the Mind Stone in Vision's forehead. Rogers, Natasha Romanoff, and Sam Wilson rescue them and take shelter with James Rhodes and Banner at the Avengers Facility. Vision offers to sacrifice himself by having Maximoff destroy the Mind Stone to keep Thanos from retrieving it. Rogers suggests they travel to Wakanda, which he believes has the resources to remove the stone without destroying Vision. The Guardians of the Galaxy respond to a distress call from the Asgardian ship and rescue Thor, who surmises that Thanos seeks the Reality Stone, which is in the possession of the Collector on Knowhere. Rocket and Groot accompany Thor to Nidavellir, where they and Eitri create Stormbreaker, a battle-axe capable of killing Thanos. On Knowhere, Peter Quill, Gamora, Drax, and Mantis find Thanos with the Reality Stone already in his possession. Thanos kidnaps Gamora, his adopted daughter, who reveals the location of the Soul Stone to save her captive adopted sister Nebula from torture. Thanos and Gamora travel to Vormir, where Red Skull, keeper of the Soul Stone, informs him the stone can only be retrieved by sacrificing someone he loves. Thanos reluctantly kills Gamora, earning the stone. Nebula escapes captivity and asks the remaining Guardians to meet her on Thanos' destroyed homeworld, Titan. Stark and Parker kill Maw and rescue Strange. Landing on Titan, they meet Quill, Drax, and Mantis. The group forms a plan to remove Thanos' Infinity Gauntlet after Strange uses the Time Stone to view millions of possible futures, seeing only one in which Thanos loses. Thanos arrives, justifying his plans as necessary to ensure the survival of a universe threatened by overpopulation. The group subdues him until Nebula deduces that Thanos has killed Gamora. Enraged, Quill attacks him, allowing Thanos to break the group's hold and overpower them. Stark is seriously wounded by Thanos, but is spared after Strange surrenders the Time Stone to Thanos. In Wakanda, Rogers reunites with Bucky Barnes before Thanos' army invades. The Avengers, alongside T'Challa and the Wakandan forces, mount a defense while Shuri works to extract the Mind Stone from Vision. Banner, unable to transform into the Hulk, fights in Stark's Hulkbuster armor. Thor, Rocket, and Groot arrive to reinforce the Avengers; Midnight, Obsidian, and Glaive are killed and their army is routed. Thanos arrives and despite Maximoff's attempt to destroy the Mind Stone, removes it from Vision's head, killing him.Thor severely wounds Thanos, but Thanos activates the completed Infinity Gauntlet and teleports away. Half of all life across the universe disintegrates, including Barnes, T'Challa, Groot, Maximoff, Wilson, Mantis, Drax, Quill, Strange, and Parker, as well as Maria Hill and Nick Fury, although Fury is able to transmit a signal to Carol Danvers first. Stark and Nebula remain on Titan while Banner, M'Baku, Okoye, Rhodes, Rocket, Rogers, Romanoff, and Thor are left on the Wakandan battlefield. Meanwhile, Thanos watches a sunrise on another planet.\"\n```\n\nIn the code below I try splitting the model up first by sentence then run a `for` loop to train the model per sentence.\n\n\n```python\nchain = MarkovChain()\nfor sentence in training_text.split('.'):\n    chain.train_model(sentence)\n```\n\nLet's see what the model comes up with when we generate 10 sentences that are 20 words long:\n\n\n```python\nfor index in range(1,11):\n    print(str(index)+'. '+chain.generate_text(20))\n```\n\n    1. wanda maximoff and sam wilson rescue strange recruits tony stark is able to thanos to earth using the sanctum sanctorum in\n    2. midnight, and wong stays behind to earth using the soul stone, one of the universe disintegrates, including barnes, t'challa, groot, maximoff,\n    3. rescue them and despite maximoff's attempt to view millions of the tesseract, thanos departs with james rhodes and mantis find thanos\n    4. knowhere, peter quill, gamora, his lieutenants—ebony maw, cull obsidian, proxima midnight, obsidian, and parker, as they travel to reinforce the remaining\n    5. which is seriously wounded by overpopulation\n    6. distress call from vision's forehead\n    7. group's hold and teleports away\n    8. fury is seriously wounded by thanos, but fails to an enchantment\n    9. acquired the possession of the collector on thanos' destroyed homeworld, titan while banner, m'baku, okoye, rhodes, rocket, rogers, and rescue strange\n    10. remove the time stone without destroying vision in new york city, reverting to thanos and groot arrive to guard the ship\n\n\nNow let's try training the model by feeding it the whole `training_text` corpus instead and generate sentences that end in a period (.):\n\n\n```python\nchain = MarkovChain()\nchain.train_model(training_text)    \nfor index in range(1,11):\n    print(str(index)+'. '+chain.generate_sentence())\n```\n\n    1. universe; in stark's hulkbuster armor.\n    2. be retrieved by sacrificing someone he believes has killed and despite maximoff's attempt to remove thanos' plan to transform into the time stone to earth using the group forms a distress call from strange, drawing the wakandan battlefield.\n    3. midnight, and parker kill half of killing thanos.\n    4. all life in vision's forehead.\n    5. survival of killing him.thor\n    6. but thanos seeks the asgardian ship and nick fury, although fury is seriously wounded by overpopulation.\n    7. thor to save her on titan, they and asks the sanctum.\n    8. distress call from strange, drawing the planet xandar, thanos arrives and gamora travel to earth using the possession of peter parker.\n    9. wilson rescue strange.\n    10. knowhere.\n\n\nHopefully you found the sentences generated by the Markov Chain amusing :D. For more information about n-grams and Markov Chain, just do a regular Google search and that would lead you to the right direction!\n\n","source":"_posts/markov_chain.md","raw":"---\ntitle: The one about N-Grams and Markov Chains\ndate: 2019-04-21 08:30:00\ntags: \n    - python\n    - numpy\n    - n-grams\n    - nlp\n    - markov chains\n---\nToday we are going to make a Markov Chain to generate text from a training dataset we are going to feed it. So how do we start? We start by first implementing n-grams. N-grams are a list of n words that share a common border in a given string. It may sound confusing, so it is better that I implement it first then show by example:\n\n\n```python\nclass NGram:\n\n    def ngram(self, text, n=2):\n        text_list = text.lower().split()\n        grams = [tuple(text_list[index:index+n]) for index in range(0, len(text_list) - (n-1))]\n        return grams\n\n    def bigram(self, text):\n        return self.ngram(text, n=2)\n\n    def trigram(self, text):\n        return self.ngram(text, n=3)\n```\n\nI defined a class called `NGram` that contains three functions. The first function `ngram` takes in a string variable `text` and an integer `n` to create an n-gram. By default, if no `n` is passed, then `n=2` and we get what's called a bigram (bi meaning 2). In line 4 I do some simple preprocessing of the text by converting everything to lowercase with the `lower` function and then splitting the string per word into a list with the `split` function (if you don't specify a separator in the `split` function, it splits the string by whitespace). n-grams are actually generated in line 5 where I generate a list of `tuples` using list indexing and list comprehension techniques.\n\n`bigram` and `trigram` functions are used to make n-grams where `n=2` and `n=3` respectively. As you can see, they use the `ngram` function under the hood. Lets have a few examples so you can see what the piece of code does on out sample text: \"Hello word, this is a sample text for consumption of the NGram class!\"\n\n\n```python\nsample = \"Hello word, this is a sample text for consumption of the NGram class!\"\nngram = NGram()\n# bi-gram example\nprint(ngram.bigram(sample))\n```\n\n    [('hello', 'word,'), ('word,', 'this'), ('this', 'is'), ('is', 'a'), ('a', 'sample'), ('sample', 'text'), ('text', 'for'), ('for', 'consumption'), ('consumption', 'of'), ('of', 'the'), ('the', 'ngram'), ('ngram', 'class!')]\n\n\n\n```python\n# tri-gram example\nprint(ngram.trigram(sample))\n```\n\n    [('hello', 'word,', 'this'), ('word,', 'this', 'is'), ('this', 'is', 'a'), ('is', 'a', 'sample'), ('a', 'sample', 'text'), ('sample', 'text', 'for'), ('text', 'for', 'consumption'), ('for', 'consumption', 'of'), ('consumption', 'of', 'the'), ('of', 'the', 'ngram'), ('the', 'ngram', 'class!')]\n\n\n\n```python\n# n-gram example where n=4\nprint(ngram.ngram(sample,n=4))\n```\n\n    [('hello', 'word,', 'this', 'is'), ('word,', 'this', 'is', 'a'), ('this', 'is', 'a', 'sample'), ('is', 'a', 'sample', 'text'), ('a', 'sample', 'text', 'for'), ('sample', 'text', 'for', 'consumption'), ('text', 'for', 'consumption', 'of'), ('for', 'consumption', 'of', 'the'), ('consumption', 'of', 'the', 'ngram'), ('of', 'the', 'ngram', 'class!')]\n\n\nI do hope the examples above made n-grams clearer than I could explain them :). Now that we've implemented our NGram class, we can start building our shiny Markov Chain! The markov chain class is a bit lengthy, so I chose to write docstrings under the function names instead so you have have a high level view of the class and dig in to investigate the code if you want.\n\n\n```python\nimport numpy as np\n\nclass MarkovChain:\n    \n    def __init__(self):\n        \"\"\"\n        Initialization creates empty dictionary\n        \"\"\"\n        self.model = {}\n    \n    def calculate_probabilities(self):\n        \"\"\"\n        per word in model, calculate the probability of next words following the current word\n        based on the bigram pair and how many times the bigram pair appears in the training dataset\n        \"\"\"\n        for key in self.model:\n            total_sum = np.sum(self.model[key]['counts'])\n            self.model[key]['probability'] = [x/total_sum for x in self.model[key]['counts']]\n            \n    def train_model(self,text):\n        \"\"\"\n        The train_model method first generates bigrams from the given text string. The method then\n        traverses each bigram and determines if the first word in the bigram already exists in the model.\n        The behavior of the method per bigram is determined with the following cases:\n        \n        If the first element has not been added yet(does not have a key in the dictionary), the first \n        element is added to the dictionary with the second element added in the `values` list with a \n        corresponding occurance count of 1.\n        \n        If the first element has already been added but the second element has not yet been added to the\n        `values` list, it appends the second element to the `values` list with a corresponding occurance count \n        of 1. \n        \n        If both the first and second elements of the bigram already exist, just increment the number of\n        occurances of the second element by 1.\n        \n        After all bigrams have been processed, the train_model method calls the calculate_probability method\n        to calculate the probability of choosing the next word randomly given the current work in the Markov\n        Chain.\n        \n        The final model would have the following structure:\n        \n        {\n            'word-1': {\n                values: [<list_of_probable_words>]\n                counts: [<number_of_occurences_per_word_in_values>]\n                probability: [<probability_of_occurence_per_word_in_values>]\n            },\n            'word-2': {\n                values: [<list_of_probable_words>]\n                counts: [<number_of_occurences_per_word_in_values>]\n                probability: [<probability_of_occurence_per_word_in_values>]\n            },\n            ...,\n            'word-n': {\n                values: [<list_of_probable_words>]\n                counts: [<number_of_occurences_per_word_in_values>]\n                probability: [<probability_of_occurence_per_word_in_values>]\n            }\n        }\n        \"\"\"\n        ngram = NGram()\n        \n        bigrams = ngram.bigram(text)\n        \n        for bigram in bigrams:\n            if bigram[0] not in self.model:\n                self.model[bigram[0]]= {\n                    'values': [bigram[1]],\n                    'counts': [1],\n                    'probability': []\n                }\n            else:\n                if bigram[1] not in self.model[bigram[0]]['values']:\n                    self.model[bigram[0]]['values'].append(bigram[1])\n                    self.model[bigram[0]]['counts'].append(1)\n                else:\n                    index = self.model[bigram[0]]['values'].index(bigram[1])\n                    self.model[bigram[0]]['counts'][index] = self.model[bigram[0]]['counts'][index] + 1\n            \n            self.calculate_probabilities()\n    \n    def generate_text(self, n):\n        \"\"\"\n        This method generates a random sentence of n words. It selects a random starting point from the model\n        then chooses a random hop to the next word based on the probability array of the word list n times.\n        \n        Note that it is possible to have a word that does not have next hops, we gracefully handle that with\n        a try except case that returns the sentence prematurely if ever that happens.\n        \"\"\"\n        sentence = ''\n        current_word = np.random.choice(list(self.model.keys()))\n        sentence += current_word\n        for index in range(n):\n            try:\n                current_word = np.random.choice(self.model[current_word]['values'],p=self.model[current_word]['probability'])\n                sentence += ' '+current_word\n            except:\n                return sentence   \n        return sentence\n    \n    def generate_sentence(self):\n        \"\"\"\n        This method generates a random sentence of . It selects a random starting point from the model\n        then chooses a random hop to the next word based on the probability array of the word list until\n        it encounters a \".\".\n        \n        Note that it is possible to have a word that does not have next hops, we gracefully handle that with\n        a try except case that returns the sentence prematurely if ever that happens.\n        \"\"\"\n        sentence = ''\n        current_word = np.random.choice(list(self.model.keys()))\n        sentence += current_word\n        while '.' not in current_word:\n            try:\n                current_word = np.random.choice(self.model[current_word]['values'],p=self.model[current_word]['probability'])\n                sentence += ' '+current_word\n            except:\n                return sentence   \n        return sentence\n    \n```\n\nNow that we have the Markov Chain class implemented, we can test out the code! For the training dataset, I chose wikipedia's summary of the [Avengers: infinity war](https://en.wikipedia.org/wiki/Avengers:_Infinity_War#Plot) movie.\n\n\n```python\n\ntraining_text = \"Having acquired the Power Stone, one of the six Infinity Stones, from the planet Xandar, Thanos and his lieutenants—Ebony Maw, Cull Obsidian, Proxima Midnight, and Corvus Glaive—intercept the spaceship carrying the surviving Asgardians. As they extract the Space Stone from the Tesseract, Thanos subdues Thor, overpowers Hulk, and kills Loki. Heimdall sends Hulk to Earth using the Bifröst before being killed. Thanos departs with his lieutenants and obliterates the ship. Hulk crash-lands at the Sanctum Sanctorum in New York City, reverting to Bruce Banner. He warns Stephen Strange and Wong about Thanos' plan to kill half of all life in the universe; in response, Strange recruits Tony Stark. Maw and Obsidian arrive to retrieve the Time Stone from Strange, drawing the attention of Peter Parker. Maw captures Strange, but fails to take the Time Stone due to an enchantment. Stark and Parker pursue Maw's spaceship, Banner contacts Steve Rogers, and Wong stays behind to guard the Sanctum. In Edinburgh, Midnight and Glaive ambush Wanda Maximoff and Vision in order to retrieve the Mind Stone in Vision's forehead. Rogers, Natasha Romanoff, and Sam Wilson rescue them and take shelter with James Rhodes and Banner at the Avengers Facility. Vision offers to sacrifice himself by having Maximoff destroy the Mind Stone to keep Thanos from retrieving it. Rogers suggests they travel to Wakanda, which he believes has the resources to remove the stone without destroying Vision. The Guardians of the Galaxy respond to a distress call from the Asgardian ship and rescue Thor, who surmises that Thanos seeks the Reality Stone, which is in the possession of the Collector on Knowhere. Rocket and Groot accompany Thor to Nidavellir, where they and Eitri create Stormbreaker, a battle-axe capable of killing Thanos. On Knowhere, Peter Quill, Gamora, Drax, and Mantis find Thanos with the Reality Stone already in his possession. Thanos kidnaps Gamora, his adopted daughter, who reveals the location of the Soul Stone to save her captive adopted sister Nebula from torture. Thanos and Gamora travel to Vormir, where Red Skull, keeper of the Soul Stone, informs him the stone can only be retrieved by sacrificing someone he loves. Thanos reluctantly kills Gamora, earning the stone. Nebula escapes captivity and asks the remaining Guardians to meet her on Thanos' destroyed homeworld, Titan. Stark and Parker kill Maw and rescue Strange. Landing on Titan, they meet Quill, Drax, and Mantis. The group forms a plan to remove Thanos' Infinity Gauntlet after Strange uses the Time Stone to view millions of possible futures, seeing only one in which Thanos loses. Thanos arrives, justifying his plans as necessary to ensure the survival of a universe threatened by overpopulation. The group subdues him until Nebula deduces that Thanos has killed Gamora. Enraged, Quill attacks him, allowing Thanos to break the group's hold and overpower them. Stark is seriously wounded by Thanos, but is spared after Strange surrenders the Time Stone to Thanos. In Wakanda, Rogers reunites with Bucky Barnes before Thanos' army invades. The Avengers, alongside T'Challa and the Wakandan forces, mount a defense while Shuri works to extract the Mind Stone from Vision. Banner, unable to transform into the Hulk, fights in Stark's Hulkbuster armor. Thor, Rocket, and Groot arrive to reinforce the Avengers; Midnight, Obsidian, and Glaive are killed and their army is routed. Thanos arrives and despite Maximoff's attempt to destroy the Mind Stone, removes it from Vision's head, killing him.Thor severely wounds Thanos, but Thanos activates the completed Infinity Gauntlet and teleports away. Half of all life across the universe disintegrates, including Barnes, T'Challa, Groot, Maximoff, Wilson, Mantis, Drax, Quill, Strange, and Parker, as well as Maria Hill and Nick Fury, although Fury is able to transmit a signal to Carol Danvers first. Stark and Nebula remain on Titan while Banner, M'Baku, Okoye, Rhodes, Rocket, Rogers, Romanoff, and Thor are left on the Wakandan battlefield. Meanwhile, Thanos watches a sunrise on another planet.\"\n```\n\nIn the code below I try splitting the model up first by sentence then run a `for` loop to train the model per sentence.\n\n\n```python\nchain = MarkovChain()\nfor sentence in training_text.split('.'):\n    chain.train_model(sentence)\n```\n\nLet's see what the model comes up with when we generate 10 sentences that are 20 words long:\n\n\n```python\nfor index in range(1,11):\n    print(str(index)+'. '+chain.generate_text(20))\n```\n\n    1. wanda maximoff and sam wilson rescue strange recruits tony stark is able to thanos to earth using the sanctum sanctorum in\n    2. midnight, and wong stays behind to earth using the soul stone, one of the universe disintegrates, including barnes, t'challa, groot, maximoff,\n    3. rescue them and despite maximoff's attempt to view millions of the tesseract, thanos departs with james rhodes and mantis find thanos\n    4. knowhere, peter quill, gamora, his lieutenants—ebony maw, cull obsidian, proxima midnight, obsidian, and parker, as they travel to reinforce the remaining\n    5. which is seriously wounded by overpopulation\n    6. distress call from vision's forehead\n    7. group's hold and teleports away\n    8. fury is seriously wounded by thanos, but fails to an enchantment\n    9. acquired the possession of the collector on thanos' destroyed homeworld, titan while banner, m'baku, okoye, rhodes, rocket, rogers, and rescue strange\n    10. remove the time stone without destroying vision in new york city, reverting to thanos and groot arrive to guard the ship\n\n\nNow let's try training the model by feeding it the whole `training_text` corpus instead and generate sentences that end in a period (.):\n\n\n```python\nchain = MarkovChain()\nchain.train_model(training_text)    \nfor index in range(1,11):\n    print(str(index)+'. '+chain.generate_sentence())\n```\n\n    1. universe; in stark's hulkbuster armor.\n    2. be retrieved by sacrificing someone he believes has killed and despite maximoff's attempt to remove thanos' plan to transform into the time stone to earth using the group forms a distress call from strange, drawing the wakandan battlefield.\n    3. midnight, and parker kill half of killing thanos.\n    4. all life in vision's forehead.\n    5. survival of killing him.thor\n    6. but thanos seeks the asgardian ship and nick fury, although fury is seriously wounded by overpopulation.\n    7. thor to save her on titan, they and asks the sanctum.\n    8. distress call from strange, drawing the planet xandar, thanos arrives and gamora travel to earth using the possession of peter parker.\n    9. wilson rescue strange.\n    10. knowhere.\n\n\nHopefully you found the sentences generated by the Markov Chain amusing :D. For more information about n-grams and Markov Chain, just do a regular Google search and that would lead you to the right direction!\n\n","slug":"markov_chain","published":1,"updated":"2019-04-21T07:06:46.639Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjxg5gmea001d0lfo9e959ilg","content":"<p>Today we are going to make a Markov Chain to generate text from a training dataset we are going to feed it. So how do we start? We start by first implementing n-grams. N-grams are a list of n words that share a common border in a given string. It may sound confusing, so it is better that I implement it first then show by example:</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">NGram</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">ngram</span><span class=\"params\">(self, text, n=<span class=\"number\">2</span>)</span>:</span></span><br><span class=\"line\">        text_list = text.lower().split()</span><br><span class=\"line\">        grams = [tuple(text_list[index:index+n]) <span class=\"keyword\">for</span> index <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, len(text_list) - (n<span class=\"number\">-1</span>))]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> grams</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bigram</span><span class=\"params\">(self, text)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.ngram(text, n=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">trigram</span><span class=\"params\">(self, text)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.ngram(text, n=<span class=\"number\">3</span>)</span><br></pre></td></tr></tbody></table></figure>\n<p>I defined a class called <code>NGram</code> that contains three functions. The first function <code>ngram</code> takes in a string variable <code>text</code> and an integer <code>n</code> to create an n-gram. By default, if no <code>n</code> is passed, then <code>n=2</code> and we get what&#x2019;s called a bigram (bi meaning 2). In line 4 I do some simple preprocessing of the text by converting everything to lowercase with the <code>lower</code> function and then splitting the string per word into a list with the <code>split</code> function (if you don&#x2019;t specify a separator in the <code>split</code> function, it splits the string by whitespace). n-grams are actually generated in line 5 where I generate a list of <code>tuples</code> using list indexing and list comprehension techniques.</p>\n<p><code>bigram</code> and <code>trigram</code> functions are used to make n-grams where <code>n=2</code> and <code>n=3</code> respectively. As you can see, they use the <code>ngram</code> function under the hood. Lets have a few examples so you can see what the piece of code does on out sample text: &#x201C;Hello word, this is a sample text for consumption of the NGram class!&#x201D;</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sample = <span class=\"string\">&quot;Hello word, this is a sample text for consumption of the NGram class!&quot;</span></span><br><span class=\"line\">ngram = NGram()</span><br><span class=\"line\"><span class=\"comment\"># bi-gram example</span></span><br><span class=\"line\">print(ngram.bigram(sample))</span><br></pre></td></tr></tbody></table></figure>\n<pre><code>[(&apos;hello&apos;, &apos;word,&apos;), (&apos;word,&apos;, &apos;this&apos;), (&apos;this&apos;, &apos;is&apos;), (&apos;is&apos;, &apos;a&apos;), (&apos;a&apos;, &apos;sample&apos;), (&apos;sample&apos;, &apos;text&apos;), (&apos;text&apos;, &apos;for&apos;), (&apos;for&apos;, &apos;consumption&apos;), (&apos;consumption&apos;, &apos;of&apos;), (&apos;of&apos;, &apos;the&apos;), (&apos;the&apos;, &apos;ngram&apos;), (&apos;ngram&apos;, &apos;class!&apos;)]\n</code></pre><figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># tri-gram example</span></span><br><span class=\"line\">print(ngram.trigram(sample))</span><br></pre></td></tr></tbody></table></figure>\n<pre><code>[(&apos;hello&apos;, &apos;word,&apos;, &apos;this&apos;), (&apos;word,&apos;, &apos;this&apos;, &apos;is&apos;), (&apos;this&apos;, &apos;is&apos;, &apos;a&apos;), (&apos;is&apos;, &apos;a&apos;, &apos;sample&apos;), (&apos;a&apos;, &apos;sample&apos;, &apos;text&apos;), (&apos;sample&apos;, &apos;text&apos;, &apos;for&apos;), (&apos;text&apos;, &apos;for&apos;, &apos;consumption&apos;), (&apos;for&apos;, &apos;consumption&apos;, &apos;of&apos;), (&apos;consumption&apos;, &apos;of&apos;, &apos;the&apos;), (&apos;of&apos;, &apos;the&apos;, &apos;ngram&apos;), (&apos;the&apos;, &apos;ngram&apos;, &apos;class!&apos;)]\n</code></pre><figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># n-gram example where n=4</span></span><br><span class=\"line\">print(ngram.ngram(sample,n=<span class=\"number\">4</span>))</span><br></pre></td></tr></tbody></table></figure>\n<pre><code>[(&apos;hello&apos;, &apos;word,&apos;, &apos;this&apos;, &apos;is&apos;), (&apos;word,&apos;, &apos;this&apos;, &apos;is&apos;, &apos;a&apos;), (&apos;this&apos;, &apos;is&apos;, &apos;a&apos;, &apos;sample&apos;), (&apos;is&apos;, &apos;a&apos;, &apos;sample&apos;, &apos;text&apos;), (&apos;a&apos;, &apos;sample&apos;, &apos;text&apos;, &apos;for&apos;), (&apos;sample&apos;, &apos;text&apos;, &apos;for&apos;, &apos;consumption&apos;), (&apos;text&apos;, &apos;for&apos;, &apos;consumption&apos;, &apos;of&apos;), (&apos;for&apos;, &apos;consumption&apos;, &apos;of&apos;, &apos;the&apos;), (&apos;consumption&apos;, &apos;of&apos;, &apos;the&apos;, &apos;ngram&apos;), (&apos;of&apos;, &apos;the&apos;, &apos;ngram&apos;, &apos;class!&apos;)]\n</code></pre><p>I do hope the examples above made n-grams clearer than I could explain them :). Now that we&#x2019;ve implemented our NGram class, we can start building our shiny Markov Chain! The markov chain class is a bit lengthy, so I chose to write docstrings under the function names instead so you have have a high level view of the class and dig in to investigate the code if you want.</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MarkovChain</span>:</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        Initialization creates empty dictionary</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        self.model = {}</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calculate_probabilities</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        per word in model, calculate the probability of next words following the current word</span></span><br><span class=\"line\"><span class=\"string\">        based on the bigram pair and how many times the bigram pair appears in the training dataset</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> self.model:</span><br><span class=\"line\">            total_sum = np.sum(self.model[key][<span class=\"string\">&apos;counts&apos;</span>])</span><br><span class=\"line\">            self.model[key][<span class=\"string\">&apos;probability&apos;</span>] = [x/total_sum <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> self.model[key][<span class=\"string\">&apos;counts&apos;</span>]]</span><br><span class=\"line\">            </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_model</span><span class=\"params\">(self,text)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        The train_model method first generates bigrams from the given text string. The method then</span></span><br><span class=\"line\"><span class=\"string\">        traverses each bigram and determines if the first word in the bigram already exists in the model.</span></span><br><span class=\"line\"><span class=\"string\">        The behavior of the method per bigram is determined with the following cases:</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        If the first element has not been added yet(does not have a key in the dictionary), the first </span></span><br><span class=\"line\"><span class=\"string\">        element is added to the dictionary with the second element added in the `values` list with a </span></span><br><span class=\"line\"><span class=\"string\">        corresponding occurance count of 1.</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        If the first element has already been added but the second element has not yet been added to the</span></span><br><span class=\"line\"><span class=\"string\">        `values` list, it appends the second element to the `values` list with a corresponding occurance count </span></span><br><span class=\"line\"><span class=\"string\">        of 1. </span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        If both the first and second elements of the bigram already exist, just increment the number of</span></span><br><span class=\"line\"><span class=\"string\">        occurances of the second element by 1.</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        After all bigrams have been processed, the train_model method calls the calculate_probability method</span></span><br><span class=\"line\"><span class=\"string\">        to calculate the probability of choosing the next word randomly given the current work in the Markov</span></span><br><span class=\"line\"><span class=\"string\">        Chain.</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        The final model would have the following structure:</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        {</span></span><br><span class=\"line\"><span class=\"string\">            &apos;word-1&apos;: {</span></span><br><span class=\"line\"><span class=\"string\">                values: [&lt;list_of_probable_words&gt;]</span></span><br><span class=\"line\"><span class=\"string\">                counts: [&lt;number_of_occurences_per_word_in_values&gt;]</span></span><br><span class=\"line\"><span class=\"string\">                probability: [&lt;probability_of_occurence_per_word_in_values&gt;]</span></span><br><span class=\"line\"><span class=\"string\">            },</span></span><br><span class=\"line\"><span class=\"string\">            &apos;word-2&apos;: {</span></span><br><span class=\"line\"><span class=\"string\">                values: [&lt;list_of_probable_words&gt;]</span></span><br><span class=\"line\"><span class=\"string\">                counts: [&lt;number_of_occurences_per_word_in_values&gt;]</span></span><br><span class=\"line\"><span class=\"string\">                probability: [&lt;probability_of_occurence_per_word_in_values&gt;]</span></span><br><span class=\"line\"><span class=\"string\">            },</span></span><br><span class=\"line\"><span class=\"string\">            ...,</span></span><br><span class=\"line\"><span class=\"string\">            &apos;word-n&apos;: {</span></span><br><span class=\"line\"><span class=\"string\">                values: [&lt;list_of_probable_words&gt;]</span></span><br><span class=\"line\"><span class=\"string\">                counts: [&lt;number_of_occurences_per_word_in_values&gt;]</span></span><br><span class=\"line\"><span class=\"string\">                probability: [&lt;probability_of_occurence_per_word_in_values&gt;]</span></span><br><span class=\"line\"><span class=\"string\">            }</span></span><br><span class=\"line\"><span class=\"string\">        }</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        ngram = NGram()</span><br><span class=\"line\">        </span><br><span class=\"line\">        bigrams = ngram.bigram(text)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">for</span> bigram <span class=\"keyword\">in</span> bigrams:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> bigram[<span class=\"number\">0</span>] <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> self.model:</span><br><span class=\"line\">                self.model[bigram[<span class=\"number\">0</span>]]= {</span><br><span class=\"line\">                    <span class=\"string\">&apos;values&apos;</span>: [bigram[<span class=\"number\">1</span>]],</span><br><span class=\"line\">                    <span class=\"string\">&apos;counts&apos;</span>: [<span class=\"number\">1</span>],</span><br><span class=\"line\">                    <span class=\"string\">&apos;probability&apos;</span>: []</span><br><span class=\"line\">                }</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                <span class=\"keyword\">if</span> bigram[<span class=\"number\">1</span>] <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> self.model[bigram[<span class=\"number\">0</span>]][<span class=\"string\">&apos;values&apos;</span>]:</span><br><span class=\"line\">                    self.model[bigram[<span class=\"number\">0</span>]][<span class=\"string\">&apos;values&apos;</span>].append(bigram[<span class=\"number\">1</span>])</span><br><span class=\"line\">                    self.model[bigram[<span class=\"number\">0</span>]][<span class=\"string\">&apos;counts&apos;</span>].append(<span class=\"number\">1</span>)</span><br><span class=\"line\">                <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                    index = self.model[bigram[<span class=\"number\">0</span>]][<span class=\"string\">&apos;values&apos;</span>].index(bigram[<span class=\"number\">1</span>])</span><br><span class=\"line\">                    self.model[bigram[<span class=\"number\">0</span>]][<span class=\"string\">&apos;counts&apos;</span>][index] = self.model[bigram[<span class=\"number\">0</span>]][<span class=\"string\">&apos;counts&apos;</span>][index] + <span class=\"number\">1</span></span><br><span class=\"line\">            </span><br><span class=\"line\">            self.calculate_probabilities()</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_text</span><span class=\"params\">(self, n)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        This method generates a random sentence of n words. It selects a random starting point from the model</span></span><br><span class=\"line\"><span class=\"string\">        then chooses a random hop to the next word based on the probability array of the word list n times.</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        Note that it is possible to have a word that does not have next hops, we gracefully handle that with</span></span><br><span class=\"line\"><span class=\"string\">        a try except case that returns the sentence prematurely if ever that happens.</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        sentence = <span class=\"string\">&apos;&apos;</span></span><br><span class=\"line\">        current_word = np.random.choice(list(self.model.keys()))</span><br><span class=\"line\">        sentence += current_word</span><br><span class=\"line\">        <span class=\"keyword\">for</span> index <span class=\"keyword\">in</span> range(n):</span><br><span class=\"line\">            <span class=\"keyword\">try</span>:</span><br><span class=\"line\">                current_word = np.random.choice(self.model[current_word][<span class=\"string\">&apos;values&apos;</span>],p=self.model[current_word][<span class=\"string\">&apos;probability&apos;</span>])</span><br><span class=\"line\">                sentence += <span class=\"string\">&apos; &apos;</span>+current_word</span><br><span class=\"line\">            <span class=\"keyword\">except</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> sentence   </span><br><span class=\"line\">        <span class=\"keyword\">return</span> sentence</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_sentence</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        This method generates a random sentence of . It selects a random starting point from the model</span></span><br><span class=\"line\"><span class=\"string\">        then chooses a random hop to the next word based on the probability array of the word list until</span></span><br><span class=\"line\"><span class=\"string\">        it encounters a &quot;.&quot;.</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        Note that it is possible to have a word that does not have next hops, we gracefully handle that with</span></span><br><span class=\"line\"><span class=\"string\">        a try except case that returns the sentence prematurely if ever that happens.</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        sentence = <span class=\"string\">&apos;&apos;</span></span><br><span class=\"line\">        current_word = np.random.choice(list(self.model.keys()))</span><br><span class=\"line\">        sentence += current_word</span><br><span class=\"line\">        <span class=\"keyword\">while</span> <span class=\"string\">&apos;.&apos;</span> <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> current_word:</span><br><span class=\"line\">            <span class=\"keyword\">try</span>:</span><br><span class=\"line\">                current_word = np.random.choice(self.model[current_word][<span class=\"string\">&apos;values&apos;</span>],p=self.model[current_word][<span class=\"string\">&apos;probability&apos;</span>])</span><br><span class=\"line\">                sentence += <span class=\"string\">&apos; &apos;</span>+current_word</span><br><span class=\"line\">            <span class=\"keyword\">except</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> sentence   </span><br><span class=\"line\">        <span class=\"keyword\">return</span> sentence</span><br></pre></td></tr></tbody></table></figure>\n<p>Now that we have the Markov Chain class implemented, we can test out the code! For the training dataset, I chose wikipedia&#x2019;s summary of the <a href=\"https://en.wikipedia.org/wiki/Avengers:_Infinity_War#Plot\" target=\"_blank\" rel=\"noopener\">Avengers: infinity war</a> movie.</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">training_text = <span class=\"string\">&quot;Having acquired the Power Stone, one of the six Infinity Stones, from the planet Xandar, Thanos and his lieutenants&#x2014;Ebony Maw, Cull Obsidian, Proxima Midnight, and Corvus Glaive&#x2014;intercept the spaceship carrying the surviving Asgardians. As they extract the Space Stone from the Tesseract, Thanos subdues Thor, overpowers Hulk, and kills Loki. Heimdall sends Hulk to Earth using the Bifr&#xF6;st before being killed. Thanos departs with his lieutenants and obliterates the ship. Hulk crash-lands at the Sanctum Sanctorum in New York City, reverting to Bruce Banner. He warns Stephen Strange and Wong about Thanos&apos; plan to kill half of all life in the universe; in response, Strange recruits Tony Stark. Maw and Obsidian arrive to retrieve the Time Stone from Strange, drawing the attention of Peter Parker. Maw captures Strange, but fails to take the Time Stone due to an enchantment. Stark and Parker pursue Maw&apos;s spaceship, Banner contacts Steve Rogers, and Wong stays behind to guard the Sanctum. In Edinburgh, Midnight and Glaive ambush Wanda Maximoff and Vision in order to retrieve the Mind Stone in Vision&apos;s forehead. Rogers, Natasha Romanoff, and Sam Wilson rescue them and take shelter with James Rhodes and Banner at the Avengers Facility. Vision offers to sacrifice himself by having Maximoff destroy the Mind Stone to keep Thanos from retrieving it. Rogers suggests they travel to Wakanda, which he believes has the resources to remove the stone without destroying Vision. The Guardians of the Galaxy respond to a distress call from the Asgardian ship and rescue Thor, who surmises that Thanos seeks the Reality Stone, which is in the possession of the Collector on Knowhere. Rocket and Groot accompany Thor to Nidavellir, where they and Eitri create Stormbreaker, a battle-axe capable of killing Thanos. On Knowhere, Peter Quill, Gamora, Drax, and Mantis find Thanos with the Reality Stone already in his possession. Thanos kidnaps Gamora, his adopted daughter, who reveals the location of the Soul Stone to save her captive adopted sister Nebula from torture. Thanos and Gamora travel to Vormir, where Red Skull, keeper of the Soul Stone, informs him the stone can only be retrieved by sacrificing someone he loves. Thanos reluctantly kills Gamora, earning the stone. Nebula escapes captivity and asks the remaining Guardians to meet her on Thanos&apos; destroyed homeworld, Titan. Stark and Parker kill Maw and rescue Strange. Landing on Titan, they meet Quill, Drax, and Mantis. The group forms a plan to remove Thanos&apos; Infinity Gauntlet after Strange uses the Time Stone to view millions of possible futures, seeing only one in which Thanos loses. Thanos arrives, justifying his plans as necessary to ensure the survival of a universe threatened by overpopulation. The group subdues him until Nebula deduces that Thanos has killed Gamora. Enraged, Quill attacks him, allowing Thanos to break the group&apos;s hold and overpower them. Stark is seriously wounded by Thanos, but is spared after Strange surrenders the Time Stone to Thanos. In Wakanda, Rogers reunites with Bucky Barnes before Thanos&apos; army invades. The Avengers, alongside T&apos;Challa and the Wakandan forces, mount a defense while Shuri works to extract the Mind Stone from Vision. Banner, unable to transform into the Hulk, fights in Stark&apos;s Hulkbuster armor. Thor, Rocket, and Groot arrive to reinforce the Avengers; Midnight, Obsidian, and Glaive are killed and their army is routed. Thanos arrives and despite Maximoff&apos;s attempt to destroy the Mind Stone, removes it from Vision&apos;s head, killing him.Thor severely wounds Thanos, but Thanos activates the completed Infinity Gauntlet and teleports away. Half of all life across the universe disintegrates, including Barnes, T&apos;Challa, Groot, Maximoff, Wilson, Mantis, Drax, Quill, Strange, and Parker, as well as Maria Hill and Nick Fury, although Fury is able to transmit a signal to Carol Danvers first. Stark and Nebula remain on Titan while Banner, M&apos;Baku, Okoye, Rhodes, Rocket, Rogers, Romanoff, and Thor are left on the Wakandan battlefield. Meanwhile, Thanos watches a sunrise on another planet.&quot;</span></span><br></pre></td></tr></tbody></table></figure>\n<p>In the code below I try splitting the model up first by sentence then run a <code>for</code> loop to train the model per sentence.</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chain = MarkovChain()</span><br><span class=\"line\"><span class=\"keyword\">for</span> sentence <span class=\"keyword\">in</span> training_text.split(<span class=\"string\">&apos;.&apos;</span>):</span><br><span class=\"line\">    chain.train_model(sentence)</span><br></pre></td></tr></tbody></table></figure>\n<p>Let&#x2019;s see what the model comes up with when we generate 10 sentences that are 20 words long:</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> index <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,<span class=\"number\">11</span>):</span><br><span class=\"line\">    print(str(index)+<span class=\"string\">&apos;. &apos;</span>+chain.generate_text(<span class=\"number\">20</span>))</span><br></pre></td></tr></tbody></table></figure>\n<pre><code>1. wanda maximoff and sam wilson rescue strange recruits tony stark is able to thanos to earth using the sanctum sanctorum in\n2. midnight, and wong stays behind to earth using the soul stone, one of the universe disintegrates, including barnes, t&apos;challa, groot, maximoff,\n3. rescue them and despite maximoff&apos;s attempt to view millions of the tesseract, thanos departs with james rhodes and mantis find thanos\n4. knowhere, peter quill, gamora, his lieutenants&#x2014;ebony maw, cull obsidian, proxima midnight, obsidian, and parker, as they travel to reinforce the remaining\n5. which is seriously wounded by overpopulation\n6. distress call from vision&apos;s forehead\n7. group&apos;s hold and teleports away\n8. fury is seriously wounded by thanos, but fails to an enchantment\n9. acquired the possession of the collector on thanos&apos; destroyed homeworld, titan while banner, m&apos;baku, okoye, rhodes, rocket, rogers, and rescue strange\n10. remove the time stone without destroying vision in new york city, reverting to thanos and groot arrive to guard the ship\n</code></pre><p>Now let&#x2019;s try training the model by feeding it the whole <code>training_text</code> corpus instead and generate sentences that end in a period (.):</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chain = MarkovChain()</span><br><span class=\"line\">chain.train_model(training_text)    </span><br><span class=\"line\"><span class=\"keyword\">for</span> index <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,<span class=\"number\">11</span>):</span><br><span class=\"line\">    print(str(index)+<span class=\"string\">&apos;. &apos;</span>+chain.generate_sentence())</span><br></pre></td></tr></tbody></table></figure>\n<pre><code>1. universe; in stark&apos;s hulkbuster armor.\n2. be retrieved by sacrificing someone he believes has killed and despite maximoff&apos;s attempt to remove thanos&apos; plan to transform into the time stone to earth using the group forms a distress call from strange, drawing the wakandan battlefield.\n3. midnight, and parker kill half of killing thanos.\n4. all life in vision&apos;s forehead.\n5. survival of killing him.thor\n6. but thanos seeks the asgardian ship and nick fury, although fury is seriously wounded by overpopulation.\n7. thor to save her on titan, they and asks the sanctum.\n8. distress call from strange, drawing the planet xandar, thanos arrives and gamora travel to earth using the possession of peter parker.\n9. wilson rescue strange.\n10. knowhere.\n</code></pre><p>Hopefully you found the sentences generated by the Markov Chain amusing :D. For more information about n-grams and Markov Chain, just do a regular Google search and that would lead you to the right direction!</p>\n<script>\n        document.querySelectorAll('.github-emoji')\n          .forEach(el => {\n            if (!el.dataset.src) { return; }\n            const img = document.createElement('img');\n            img.style = 'display:none !important;';\n            img.src = el.dataset.src;\n            img.addEventListener('error', () => {\n              img.remove();\n              el.style.color = 'inherit';\n              el.style.backgroundImage = 'none';\n              el.style.background = 'none';\n            });\n            img.addEventListener('load', () => {\n              img.remove();\n            });\n            document.body.appendChild(img);\n          });\n      </script>","site":{"data":{}},"excerpt":"","more":"<p>Today we are going to make a Markov Chain to generate text from a training dataset we are going to feed it. So how do we start? We start by first implementing n-grams. N-grams are a list of n words that share a common border in a given string. It may sound confusing, so it is better that I implement it first then show by example:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">NGram</span>:</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">ngram</span><span class=\"params\">(self, text, n=<span class=\"number\">2</span>)</span>:</span></span><br><span class=\"line\">        text_list = text.lower().split()</span><br><span class=\"line\">        grams = [tuple(text_list[index:index+n]) <span class=\"keyword\">for</span> index <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, len(text_list) - (n<span class=\"number\">-1</span>))]</span><br><span class=\"line\">        <span class=\"keyword\">return</span> grams</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bigram</span><span class=\"params\">(self, text)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.ngram(text, n=<span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">trigram</span><span class=\"params\">(self, text)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> self.ngram(text, n=<span class=\"number\">3</span>)</span><br></pre></td></tr></table></figure>\n<p>I defined a class called <code>NGram</code> that contains three functions. The first function <code>ngram</code> takes in a string variable <code>text</code> and an integer <code>n</code> to create an n-gram. By default, if no <code>n</code> is passed, then <code>n=2</code> and we get what’s called a bigram (bi meaning 2). In line 4 I do some simple preprocessing of the text by converting everything to lowercase with the <code>lower</code> function and then splitting the string per word into a list with the <code>split</code> function (if you don’t specify a separator in the <code>split</code> function, it splits the string by whitespace). n-grams are actually generated in line 5 where I generate a list of <code>tuples</code> using list indexing and list comprehension techniques.</p>\n<p><code>bigram</code> and <code>trigram</code> functions are used to make n-grams where <code>n=2</code> and <code>n=3</code> respectively. As you can see, they use the <code>ngram</code> function under the hood. Lets have a few examples so you can see what the piece of code does on out sample text: “Hello word, this is a sample text for consumption of the NGram class!”</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sample = <span class=\"string\">\"Hello word, this is a sample text for consumption of the NGram class!\"</span></span><br><span class=\"line\">ngram = NGram()</span><br><span class=\"line\"><span class=\"comment\"># bi-gram example</span></span><br><span class=\"line\">print(ngram.bigram(sample))</span><br></pre></td></tr></table></figure>\n<pre><code>[(&apos;hello&apos;, &apos;word,&apos;), (&apos;word,&apos;, &apos;this&apos;), (&apos;this&apos;, &apos;is&apos;), (&apos;is&apos;, &apos;a&apos;), (&apos;a&apos;, &apos;sample&apos;), (&apos;sample&apos;, &apos;text&apos;), (&apos;text&apos;, &apos;for&apos;), (&apos;for&apos;, &apos;consumption&apos;), (&apos;consumption&apos;, &apos;of&apos;), (&apos;of&apos;, &apos;the&apos;), (&apos;the&apos;, &apos;ngram&apos;), (&apos;ngram&apos;, &apos;class!&apos;)]\n</code></pre><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># tri-gram example</span></span><br><span class=\"line\">print(ngram.trigram(sample))</span><br></pre></td></tr></table></figure>\n<pre><code>[(&apos;hello&apos;, &apos;word,&apos;, &apos;this&apos;), (&apos;word,&apos;, &apos;this&apos;, &apos;is&apos;), (&apos;this&apos;, &apos;is&apos;, &apos;a&apos;), (&apos;is&apos;, &apos;a&apos;, &apos;sample&apos;), (&apos;a&apos;, &apos;sample&apos;, &apos;text&apos;), (&apos;sample&apos;, &apos;text&apos;, &apos;for&apos;), (&apos;text&apos;, &apos;for&apos;, &apos;consumption&apos;), (&apos;for&apos;, &apos;consumption&apos;, &apos;of&apos;), (&apos;consumption&apos;, &apos;of&apos;, &apos;the&apos;), (&apos;of&apos;, &apos;the&apos;, &apos;ngram&apos;), (&apos;the&apos;, &apos;ngram&apos;, &apos;class!&apos;)]\n</code></pre><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># n-gram example where n=4</span></span><br><span class=\"line\">print(ngram.ngram(sample,n=<span class=\"number\">4</span>))</span><br></pre></td></tr></table></figure>\n<pre><code>[(&apos;hello&apos;, &apos;word,&apos;, &apos;this&apos;, &apos;is&apos;), (&apos;word,&apos;, &apos;this&apos;, &apos;is&apos;, &apos;a&apos;), (&apos;this&apos;, &apos;is&apos;, &apos;a&apos;, &apos;sample&apos;), (&apos;is&apos;, &apos;a&apos;, &apos;sample&apos;, &apos;text&apos;), (&apos;a&apos;, &apos;sample&apos;, &apos;text&apos;, &apos;for&apos;), (&apos;sample&apos;, &apos;text&apos;, &apos;for&apos;, &apos;consumption&apos;), (&apos;text&apos;, &apos;for&apos;, &apos;consumption&apos;, &apos;of&apos;), (&apos;for&apos;, &apos;consumption&apos;, &apos;of&apos;, &apos;the&apos;), (&apos;consumption&apos;, &apos;of&apos;, &apos;the&apos;, &apos;ngram&apos;), (&apos;of&apos;, &apos;the&apos;, &apos;ngram&apos;, &apos;class!&apos;)]\n</code></pre><p>I do hope the examples above made n-grams clearer than I could explain them :). Now that we’ve implemented our NGram class, we can start building our shiny Markov Chain! The markov chain class is a bit lengthy, so I chose to write docstrings under the function names instead so you have have a high level view of the class and dig in to investigate the code if you want.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MarkovChain</span>:</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        Initialization creates empty dictionary</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\">        self.model = &#123;&#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calculate_probabilities</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        per word in model, calculate the probability of next words following the current word</span></span><br><span class=\"line\"><span class=\"string\">        based on the bigram pair and how many times the bigram pair appears in the training dataset</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> self.model:</span><br><span class=\"line\">            total_sum = np.sum(self.model[key][<span class=\"string\">'counts'</span>])</span><br><span class=\"line\">            self.model[key][<span class=\"string\">'probability'</span>] = [x/total_sum <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> self.model[key][<span class=\"string\">'counts'</span>]]</span><br><span class=\"line\">            </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_model</span><span class=\"params\">(self,text)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        The train_model method first generates bigrams from the given text string. The method then</span></span><br><span class=\"line\"><span class=\"string\">        traverses each bigram and determines if the first word in the bigram already exists in the model.</span></span><br><span class=\"line\"><span class=\"string\">        The behavior of the method per bigram is determined with the following cases:</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        If the first element has not been added yet(does not have a key in the dictionary), the first </span></span><br><span class=\"line\"><span class=\"string\">        element is added to the dictionary with the second element added in the `values` list with a </span></span><br><span class=\"line\"><span class=\"string\">        corresponding occurance count of 1.</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        If the first element has already been added but the second element has not yet been added to the</span></span><br><span class=\"line\"><span class=\"string\">        `values` list, it appends the second element to the `values` list with a corresponding occurance count </span></span><br><span class=\"line\"><span class=\"string\">        of 1. </span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        If both the first and second elements of the bigram already exist, just increment the number of</span></span><br><span class=\"line\"><span class=\"string\">        occurances of the second element by 1.</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        After all bigrams have been processed, the train_model method calls the calculate_probability method</span></span><br><span class=\"line\"><span class=\"string\">        to calculate the probability of choosing the next word randomly given the current work in the Markov</span></span><br><span class=\"line\"><span class=\"string\">        Chain.</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        The final model would have the following structure:</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        &#123;</span></span><br><span class=\"line\"><span class=\"string\">            'word-1': &#123;</span></span><br><span class=\"line\"><span class=\"string\">                values: [&lt;list_of_probable_words&gt;]</span></span><br><span class=\"line\"><span class=\"string\">                counts: [&lt;number_of_occurences_per_word_in_values&gt;]</span></span><br><span class=\"line\"><span class=\"string\">                probability: [&lt;probability_of_occurence_per_word_in_values&gt;]</span></span><br><span class=\"line\"><span class=\"string\">            &#125;,</span></span><br><span class=\"line\"><span class=\"string\">            'word-2': &#123;</span></span><br><span class=\"line\"><span class=\"string\">                values: [&lt;list_of_probable_words&gt;]</span></span><br><span class=\"line\"><span class=\"string\">                counts: [&lt;number_of_occurences_per_word_in_values&gt;]</span></span><br><span class=\"line\"><span class=\"string\">                probability: [&lt;probability_of_occurence_per_word_in_values&gt;]</span></span><br><span class=\"line\"><span class=\"string\">            &#125;,</span></span><br><span class=\"line\"><span class=\"string\">            ...,</span></span><br><span class=\"line\"><span class=\"string\">            'word-n': &#123;</span></span><br><span class=\"line\"><span class=\"string\">                values: [&lt;list_of_probable_words&gt;]</span></span><br><span class=\"line\"><span class=\"string\">                counts: [&lt;number_of_occurences_per_word_in_values&gt;]</span></span><br><span class=\"line\"><span class=\"string\">                probability: [&lt;probability_of_occurence_per_word_in_values&gt;]</span></span><br><span class=\"line\"><span class=\"string\">            &#125;</span></span><br><span class=\"line\"><span class=\"string\">        &#125;</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\">        ngram = NGram()</span><br><span class=\"line\">        </span><br><span class=\"line\">        bigrams = ngram.bigram(text)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"keyword\">for</span> bigram <span class=\"keyword\">in</span> bigrams:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> bigram[<span class=\"number\">0</span>] <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> self.model:</span><br><span class=\"line\">                self.model[bigram[<span class=\"number\">0</span>]]= &#123;</span><br><span class=\"line\">                    <span class=\"string\">'values'</span>: [bigram[<span class=\"number\">1</span>]],</span><br><span class=\"line\">                    <span class=\"string\">'counts'</span>: [<span class=\"number\">1</span>],</span><br><span class=\"line\">                    <span class=\"string\">'probability'</span>: []</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                <span class=\"keyword\">if</span> bigram[<span class=\"number\">1</span>] <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> self.model[bigram[<span class=\"number\">0</span>]][<span class=\"string\">'values'</span>]:</span><br><span class=\"line\">                    self.model[bigram[<span class=\"number\">0</span>]][<span class=\"string\">'values'</span>].append(bigram[<span class=\"number\">1</span>])</span><br><span class=\"line\">                    self.model[bigram[<span class=\"number\">0</span>]][<span class=\"string\">'counts'</span>].append(<span class=\"number\">1</span>)</span><br><span class=\"line\">                <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                    index = self.model[bigram[<span class=\"number\">0</span>]][<span class=\"string\">'values'</span>].index(bigram[<span class=\"number\">1</span>])</span><br><span class=\"line\">                    self.model[bigram[<span class=\"number\">0</span>]][<span class=\"string\">'counts'</span>][index] = self.model[bigram[<span class=\"number\">0</span>]][<span class=\"string\">'counts'</span>][index] + <span class=\"number\">1</span></span><br><span class=\"line\">            </span><br><span class=\"line\">            self.calculate_probabilities()</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_text</span><span class=\"params\">(self, n)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        This method generates a random sentence of n words. It selects a random starting point from the model</span></span><br><span class=\"line\"><span class=\"string\">        then chooses a random hop to the next word based on the probability array of the word list n times.</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        Note that it is possible to have a word that does not have next hops, we gracefully handle that with</span></span><br><span class=\"line\"><span class=\"string\">        a try except case that returns the sentence prematurely if ever that happens.</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\">        sentence = <span class=\"string\">''</span></span><br><span class=\"line\">        current_word = np.random.choice(list(self.model.keys()))</span><br><span class=\"line\">        sentence += current_word</span><br><span class=\"line\">        <span class=\"keyword\">for</span> index <span class=\"keyword\">in</span> range(n):</span><br><span class=\"line\">            <span class=\"keyword\">try</span>:</span><br><span class=\"line\">                current_word = np.random.choice(self.model[current_word][<span class=\"string\">'values'</span>],p=self.model[current_word][<span class=\"string\">'probability'</span>])</span><br><span class=\"line\">                sentence += <span class=\"string\">' '</span>+current_word</span><br><span class=\"line\">            <span class=\"keyword\">except</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> sentence   </span><br><span class=\"line\">        <span class=\"keyword\">return</span> sentence</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">generate_sentence</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">        This method generates a random sentence of . It selects a random starting point from the model</span></span><br><span class=\"line\"><span class=\"string\">        then chooses a random hop to the next word based on the probability array of the word list until</span></span><br><span class=\"line\"><span class=\"string\">        it encounters a \".\".</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        Note that it is possible to have a word that does not have next hops, we gracefully handle that with</span></span><br><span class=\"line\"><span class=\"string\">        a try except case that returns the sentence prematurely if ever that happens.</span></span><br><span class=\"line\"><span class=\"string\">        \"\"\"</span></span><br><span class=\"line\">        sentence = <span class=\"string\">''</span></span><br><span class=\"line\">        current_word = np.random.choice(list(self.model.keys()))</span><br><span class=\"line\">        sentence += current_word</span><br><span class=\"line\">        <span class=\"keyword\">while</span> <span class=\"string\">'.'</span> <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> current_word:</span><br><span class=\"line\">            <span class=\"keyword\">try</span>:</span><br><span class=\"line\">                current_word = np.random.choice(self.model[current_word][<span class=\"string\">'values'</span>],p=self.model[current_word][<span class=\"string\">'probability'</span>])</span><br><span class=\"line\">                sentence += <span class=\"string\">' '</span>+current_word</span><br><span class=\"line\">            <span class=\"keyword\">except</span>:</span><br><span class=\"line\">                <span class=\"keyword\">return</span> sentence   </span><br><span class=\"line\">        <span class=\"keyword\">return</span> sentence</span><br></pre></td></tr></table></figure>\n<p>Now that we have the Markov Chain class implemented, we can test out the code! For the training dataset, I chose wikipedia’s summary of the <a href=\"https://en.wikipedia.org/wiki/Avengers:_Infinity_War#Plot\" target=\"_blank\" rel=\"noopener\">Avengers: infinity war</a> movie.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">training_text = <span class=\"string\">\"Having acquired the Power Stone, one of the six Infinity Stones, from the planet Xandar, Thanos and his lieutenants—Ebony Maw, Cull Obsidian, Proxima Midnight, and Corvus Glaive—intercept the spaceship carrying the surviving Asgardians. As they extract the Space Stone from the Tesseract, Thanos subdues Thor, overpowers Hulk, and kills Loki. Heimdall sends Hulk to Earth using the Bifröst before being killed. Thanos departs with his lieutenants and obliterates the ship. Hulk crash-lands at the Sanctum Sanctorum in New York City, reverting to Bruce Banner. He warns Stephen Strange and Wong about Thanos' plan to kill half of all life in the universe; in response, Strange recruits Tony Stark. Maw and Obsidian arrive to retrieve the Time Stone from Strange, drawing the attention of Peter Parker. Maw captures Strange, but fails to take the Time Stone due to an enchantment. Stark and Parker pursue Maw's spaceship, Banner contacts Steve Rogers, and Wong stays behind to guard the Sanctum. In Edinburgh, Midnight and Glaive ambush Wanda Maximoff and Vision in order to retrieve the Mind Stone in Vision's forehead. Rogers, Natasha Romanoff, and Sam Wilson rescue them and take shelter with James Rhodes and Banner at the Avengers Facility. Vision offers to sacrifice himself by having Maximoff destroy the Mind Stone to keep Thanos from retrieving it. Rogers suggests they travel to Wakanda, which he believes has the resources to remove the stone without destroying Vision. The Guardians of the Galaxy respond to a distress call from the Asgardian ship and rescue Thor, who surmises that Thanos seeks the Reality Stone, which is in the possession of the Collector on Knowhere. Rocket and Groot accompany Thor to Nidavellir, where they and Eitri create Stormbreaker, a battle-axe capable of killing Thanos. On Knowhere, Peter Quill, Gamora, Drax, and Mantis find Thanos with the Reality Stone already in his possession. Thanos kidnaps Gamora, his adopted daughter, who reveals the location of the Soul Stone to save her captive adopted sister Nebula from torture. Thanos and Gamora travel to Vormir, where Red Skull, keeper of the Soul Stone, informs him the stone can only be retrieved by sacrificing someone he loves. Thanos reluctantly kills Gamora, earning the stone. Nebula escapes captivity and asks the remaining Guardians to meet her on Thanos' destroyed homeworld, Titan. Stark and Parker kill Maw and rescue Strange. Landing on Titan, they meet Quill, Drax, and Mantis. The group forms a plan to remove Thanos' Infinity Gauntlet after Strange uses the Time Stone to view millions of possible futures, seeing only one in which Thanos loses. Thanos arrives, justifying his plans as necessary to ensure the survival of a universe threatened by overpopulation. The group subdues him until Nebula deduces that Thanos has killed Gamora. Enraged, Quill attacks him, allowing Thanos to break the group's hold and overpower them. Stark is seriously wounded by Thanos, but is spared after Strange surrenders the Time Stone to Thanos. In Wakanda, Rogers reunites with Bucky Barnes before Thanos' army invades. The Avengers, alongside T'Challa and the Wakandan forces, mount a defense while Shuri works to extract the Mind Stone from Vision. Banner, unable to transform into the Hulk, fights in Stark's Hulkbuster armor. Thor, Rocket, and Groot arrive to reinforce the Avengers; Midnight, Obsidian, and Glaive are killed and their army is routed. Thanos arrives and despite Maximoff's attempt to destroy the Mind Stone, removes it from Vision's head, killing him.Thor severely wounds Thanos, but Thanos activates the completed Infinity Gauntlet and teleports away. Half of all life across the universe disintegrates, including Barnes, T'Challa, Groot, Maximoff, Wilson, Mantis, Drax, Quill, Strange, and Parker, as well as Maria Hill and Nick Fury, although Fury is able to transmit a signal to Carol Danvers first. Stark and Nebula remain on Titan while Banner, M'Baku, Okoye, Rhodes, Rocket, Rogers, Romanoff, and Thor are left on the Wakandan battlefield. Meanwhile, Thanos watches a sunrise on another planet.\"</span></span><br></pre></td></tr></table></figure>\n<p>In the code below I try splitting the model up first by sentence then run a <code>for</code> loop to train the model per sentence.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chain = MarkovChain()</span><br><span class=\"line\"><span class=\"keyword\">for</span> sentence <span class=\"keyword\">in</span> training_text.split(<span class=\"string\">'.'</span>):</span><br><span class=\"line\">    chain.train_model(sentence)</span><br></pre></td></tr></table></figure>\n<p>Let’s see what the model comes up with when we generate 10 sentences that are 20 words long:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> index <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,<span class=\"number\">11</span>):</span><br><span class=\"line\">    print(str(index)+<span class=\"string\">'. '</span>+chain.generate_text(<span class=\"number\">20</span>))</span><br></pre></td></tr></table></figure>\n<pre><code>1. wanda maximoff and sam wilson rescue strange recruits tony stark is able to thanos to earth using the sanctum sanctorum in\n2. midnight, and wong stays behind to earth using the soul stone, one of the universe disintegrates, including barnes, t&apos;challa, groot, maximoff,\n3. rescue them and despite maximoff&apos;s attempt to view millions of the tesseract, thanos departs with james rhodes and mantis find thanos\n4. knowhere, peter quill, gamora, his lieutenants—ebony maw, cull obsidian, proxima midnight, obsidian, and parker, as they travel to reinforce the remaining\n5. which is seriously wounded by overpopulation\n6. distress call from vision&apos;s forehead\n7. group&apos;s hold and teleports away\n8. fury is seriously wounded by thanos, but fails to an enchantment\n9. acquired the possession of the collector on thanos&apos; destroyed homeworld, titan while banner, m&apos;baku, okoye, rhodes, rocket, rogers, and rescue strange\n10. remove the time stone without destroying vision in new york city, reverting to thanos and groot arrive to guard the ship\n</code></pre><p>Now let’s try training the model by feeding it the whole <code>training_text</code> corpus instead and generate sentences that end in a period (.):</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chain = MarkovChain()</span><br><span class=\"line\">chain.train_model(training_text)    </span><br><span class=\"line\"><span class=\"keyword\">for</span> index <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,<span class=\"number\">11</span>):</span><br><span class=\"line\">    print(str(index)+<span class=\"string\">'. '</span>+chain.generate_sentence())</span><br></pre></td></tr></table></figure>\n<pre><code>1. universe; in stark&apos;s hulkbuster armor.\n2. be retrieved by sacrificing someone he believes has killed and despite maximoff&apos;s attempt to remove thanos&apos; plan to transform into the time stone to earth using the group forms a distress call from strange, drawing the wakandan battlefield.\n3. midnight, and parker kill half of killing thanos.\n4. all life in vision&apos;s forehead.\n5. survival of killing him.thor\n6. but thanos seeks the asgardian ship and nick fury, although fury is seriously wounded by overpopulation.\n7. thor to save her on titan, they and asks the sanctum.\n8. distress call from strange, drawing the planet xandar, thanos arrives and gamora travel to earth using the possession of peter parker.\n9. wilson rescue strange.\n10. knowhere.\n</code></pre><p>Hopefully you found the sentences generated by the Markov Chain amusing :D. For more information about n-grams and Markov Chain, just do a regular Google search and that would lead you to the right direction!</p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cjxg5gmdg00000lforwqpztre","tag_id":"cjxg5gmdo00040lfo4hra6259","_id":"cjxg5gmdw000e0lfoencj9k7u"},{"post_id":"cjxg5gmdg00000lforwqpztre","tag_id":"cjxg5gmds00080lfo98ga92o0","_id":"cjxg5gmdx000f0lfozvo64ugj"},{"post_id":"cjxg5gmdg00000lforwqpztre","tag_id":"cjxg5gmdv000b0lfottoxswlf","_id":"cjxg5gmdx000h0lfo0n68a216"},{"post_id":"cjxg5gmdg00000lforwqpztre","tag_id":"cjxg5gmdw000c0lfo547gw3un","_id":"cjxg5gmdx000i0lfombeer077"},{"post_id":"cjxg5gmdl00020lfo9jao9gw1","tag_id":"cjxg5gmdw000d0lfosv08zhod","_id":"cjxg5gme2000n0lfoh59rpb8f"},{"post_id":"cjxg5gmdl00020lfo9jao9gw1","tag_id":"cjxg5gmdx000g0lfo4vnyzli0","_id":"cjxg5gme3000o0lfojfr9qyi0"},{"post_id":"cjxg5gmdl00020lfo9jao9gw1","tag_id":"cjxg5gmdy000j0lfothl9mfkg","_id":"cjxg5gme3000q0lfo11rkqwfp"},{"post_id":"cjxg5gmdl00020lfo9jao9gw1","tag_id":"cjxg5gmdz000k0lfook2d0v64","_id":"cjxg5gme3000r0lfokcif8dm7"},{"post_id":"cjxg5gmdl00020lfo9jao9gw1","tag_id":"cjxg5gme0000l0lfo4vdfkxgd","_id":"cjxg5gme4000t0lfod8uob5x6"},{"post_id":"cjxg5gmdp00050lfoems0zyjn","tag_id":"cjxg5gme1000m0lfo5zh0tjic","_id":"cjxg5gme6000v0lfowk00jmc9"},{"post_id":"cjxg5gmdp00050lfoems0zyjn","tag_id":"cjxg5gme3000p0lfo9d6k8x54","_id":"cjxg5gme6000w0lfof4iiw1u0"},{"post_id":"cjxg5gmdp00050lfoems0zyjn","tag_id":"cjxg5gme4000s0lfo5czvhczo","_id":"cjxg5gme7000y0lfoahpe2fd4"},{"post_id":"cjxg5gmds00070lfoihm4pluj","tag_id":"cjxg5gme1000m0lfo5zh0tjic","_id":"cjxg5gme700110lfot18b1i88"},{"post_id":"cjxg5gmds00070lfoihm4pluj","tag_id":"cjxg5gme3000p0lfo9d6k8x54","_id":"cjxg5gme800120lfo38y3cyar"},{"post_id":"cjxg5gmds00070lfoihm4pluj","tag_id":"cjxg5gme4000s0lfo5czvhczo","_id":"cjxg5gme800140lfotdhhgowq"},{"post_id":"cjxg5gmdt00090lfo1e3uy5g1","tag_id":"cjxg5gme700100lfo7vcqt5yc","_id":"cjxg5gme900180lfoh5gxjrr1"},{"post_id":"cjxg5gmdt00090lfo1e3uy5g1","tag_id":"cjxg5gme800130lfo0y27gpa3","_id":"cjxg5gme900190lfo2tyzus64"},{"post_id":"cjxg5gmdt00090lfo1e3uy5g1","tag_id":"cjxg5gme800150lfoki35zbt9","_id":"cjxg5gme9001a0lfofswk3rs9"},{"post_id":"cjxg5gmdt00090lfo1e3uy5g1","tag_id":"cjxg5gme800160lfoqdu0bc8l","_id":"cjxg5gme9001b0lfovcmvzt73"},{"post_id":"cjxg5gmdt00090lfo1e3uy5g1","tag_id":"cjxg5gme800170lfo6t9urmpn","_id":"cjxg5gme9001c0lfoivm9ayup"},{"post_id":"cjxg5gmea001d0lfo9e959ilg","tag_id":"cjxg5gmeb001e0lfo9u2ekdal","_id":"cjxg5gmed001j0lfo8cq24f2y"},{"post_id":"cjxg5gmea001d0lfo9e959ilg","tag_id":"cjxg5gmec001f0lfo3wutit9u","_id":"cjxg5gmed001k0lfo8dsbihlo"},{"post_id":"cjxg5gmea001d0lfo9e959ilg","tag_id":"cjxg5gmec001g0lfoeo7ndb72","_id":"cjxg5gmed001l0lfomy7r2o2o"},{"post_id":"cjxg5gmea001d0lfo9e959ilg","tag_id":"cjxg5gmec001h0lfosxd22ipl","_id":"cjxg5gmed001m0lfo7lhpqjq6"},{"post_id":"cjxg5gmea001d0lfo9e959ilg","tag_id":"cjxg5gmec001i0lfohmzadryx","_id":"cjxg5gmed001n0lfo7m3fbcjd"}],"Tag":[{"name":"c","_id":"cjxg5gmdo00040lfo4hra6259"},{"name":"c++","_id":"cjxg5gmds00080lfo98ga92o0"},{"name":"gcc","_id":"cjxg5gmdv000b0lfottoxswlf"},{"name":"compiler","_id":"cjxg5gmdw000c0lfo547gw3un"},{"name":"nvidia","_id":"cjxg5gmdw000d0lfosv08zhod"},{"name":"drivers","_id":"cjxg5gmdx000g0lfo4vnyzli0"},{"name":"ubuntu","_id":"cjxg5gmdy000j0lfothl9mfkg"},{"name":"18.04","_id":"cjxg5gmdz000k0lfook2d0v64"},{"name":"grub","_id":"cjxg5gme0000l0lfo4vdfkxgd"},{"name":"marcus aurelius","_id":"cjxg5gme1000m0lfo5zh0tjic"},{"name":"stoicism","_id":"cjxg5gme3000p0lfo9d6k8x54"},{"name":"stoic","_id":"cjxg5gme4000s0lfo5czvhczo"},{"name":"node.js","_id":"cjxg5gme700100lfo7vcqt5yc"},{"name":"cli","_id":"cjxg5gme800130lfo0y27gpa3"},{"name":"opensource","_id":"cjxg5gme800150lfoki35zbt9"},{"name":"docker","_id":"cjxg5gme800160lfoqdu0bc8l"},{"name":"docker-compose","_id":"cjxg5gme800170lfo6t9urmpn"},{"name":"python","_id":"cjxg5gmeb001e0lfo9u2ekdal"},{"name":"numpy","_id":"cjxg5gmec001f0lfo3wutit9u"},{"name":"n-grams","_id":"cjxg5gmec001g0lfoeo7ndb72"},{"name":"nlp","_id":"cjxg5gmec001h0lfosxd22ipl"},{"name":"markov chains","_id":"cjxg5gmec001i0lfohmzadryx"}]}}